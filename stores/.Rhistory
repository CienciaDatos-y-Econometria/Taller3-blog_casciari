cat("\n===============================================\n")
cat("           MÉTRICAS DEL PIPELINE\n")
cat("===============================================\n")
# 1. Obtener DTM original para comparar
dtm_original <- DocumentTermMatrix(cuentos)
# 2. Extraer números clave
docs_finales <- nrow(dtm_cuentos)
terminos_finales <- ncol(dtm_cuentos)
terminos_originales <- ncol(dtm_original)
reduccion_absoluta <- terminos_originales - terminos_finales
reduccion_pct <- round((reduccion_absoluta / terminos_originales) * 100, 1)
# 3. Top términos
frecuencias <- colSums(as.matrix(dtm_cuentos))
top_terminos <- sort(frecuencias, decreasing = TRUE)
# 4. Sparsity real
sparsity_real <- round((1 - sum(dtm_cuentos > 0) / (docs_finales * terminos_finales)) * 100, 1)
# 5. MOSTRAR TABLA PRINCIPAL
cat("DIMENSIONES Y REDUCCIÓN:\n")
cat("========================\n")
cat("Documentos finales:              ", docs_finales, "\n")
cat("Términos originales:             ", terminos_originales, "\n")
cat("Términos finales:                ", terminos_finales, "\n")
cat("Términos eliminados:             ", reduccion_absoluta, "\n")
cat("Reducción vocabulario:           ", reduccion_pct, "%\n")
cat("Sparsity aplicada:               90%\n")
cat("Sparsity real resultante:        ", sparsity_real, "%\n")
# 6. MOSTRAR TOP TÉRMINOS
cat("\nTOP 10 TÉRMINOS MÁS FRECUENTES:\n")
cat("===============================\n")
for(i in 1:min(10, length(top_terminos))) {
cat(sprintf("%2d. %-15s (%3d docs)\n", i, names(top_terminos)[i], top_terminos[i]))
}
# 7. TEXTO LISTO PARA REPORTE
cat("\n===============================================\n")
cat("           TEXTO PARA TU REPORTE\n")
cat("===============================================\n")
texto_reporte <- sprintf(
"La matriz documento-término final presenta dimensiones de %d documentos × %d términos tras aplicar un umbral de sparsity del 90%%. El vocabulario se redujo de %d a %d términos únicos, representando una disminución del %s%% que optimiza la representación semántica. Los términos más frecuentes post-procesamiento son: %s.",
docs_finales,
terminos_finales,
terminos_originales,
terminos_finales,
reduccion_pct,
paste(names(top_terminos)[1:5], collapse = ", ")
)
cat(texto_reporte, "\n")
cat("\n===============================================\n")
View(dtm_cuentos)
dtm_cuentos[["dimnames"]]
# =============================================================
# Recomendador TF-IDF + Similitud del Coseno (robusto)
# =============================================================
# Limpieza y librerías
rm(list = ls())
require(pacman)
p_load(tidyverse, tm, proxy)
# -------------------------------------------------------------
# 1) Carga de datos
# -------------------------------------------------------------
# DTM lematizada con n-gramas que generaron en el pipeline
dtm_path  <- "stores/dtm_cuentos.rds"
meta_path <- "stores/meta_cuentos.rds"  # opcional (doc_id, titulo)
if (!file.exists(dtm_path)) {
stop("No se encontró 'stores/dtm_cuentos.rds'. Ejecuta primero el pipeline.")
}
dtm <- readRDS(dtm_path)
# Cargar metadatos si existen
meta <- NULL
if (file.exists(meta_path)) {
meta <- readRDS(meta_path)
# Se espera que meta tenga una columna 'titulo' alineada con el orden de filas de la DTM
if (!is.data.frame(meta) || is.null(meta$titulo)) {
warning("El archivo 'meta_cuentos.rds' no tiene columna 'titulo'. Se ignorará meta.")
meta <- NULL
} else if (nrow(meta) != nrow(dtm)) {
warning("El número de filas de meta no coincide con la DTM. Se ignorará meta.")
meta <- NULL
}
}
# Derivar títulos: prioridad meta$titulo > Docs(dtm) > títulos de respaldo
titles <- NULL
if (!is.null(meta)) {
titles <- as.character(meta$titulo)
} else {
# tm::Docs extrae los IDs/nombres de documento si existen
titles <- tryCatch(Docs(dtm), error = function(e) NULL)
if (is.null(titles) || any(is.na(titles))) {
titles <- rownames(as.matrix(dtm))
}
if (is.null(titles) || any(is.na(titles)) || length(titles) != nrow(dtm)) {
# respaldo seguro
titles <- sprintf("doc_%04d", seq_len(nrow(dtm)))
}
}
# -------------------------------------------------------------
# 2) Construcción matriz TF-IDF normalizada y similitud coseno
# -------------------------------------------------------------
# Ponderar por TF-IDF
tfidf <- weightTfIdf(dtm)
M <- as.matrix(tfidf)
# Manejo de filas con norma cero (documentos vacíos tras preprocesamiento)
row_norm <- sqrt(rowSums(M^2))
keep <- row_norm > 0
if (!all(keep)) {
message(sprintf("Se eliminaron %d documentos con vector TF-IDF nulo.", sum(!keep)))
}
M <- M[keep, , drop = FALSE]
row_norm <- row_norm[keep]
M <- M / row_norm
titles <- titles[keep]
# Similitud de coseno (proxy::simil devuelve similitud directamente)
S <- proxy::simil(M, M, method = "cosine")
S <- as.matrix(S)  # para indexación conveniente
# -------------------------------------------------------------
# 3) Funciones de recomendación
# -------------------------------------------------------------
#' Recomendador basado en TF-IDF + coseno
#' @param title Título exacto del cuento de consulta
#' @param k número de recomendaciones a devolver
#' @return tibble con titulo_recomendado y score (similitud coseno)
recomendador_tfidf <- function(title, k = 10, titles_vec = titles, sim = S) {
stopifnot(length(title) == 1L)
idx <- match(title, titles_vec)
if (is.na(idx)) {
stop("Título no encontrado. Usa 'listar_titulos()' para ver disponibles.")
}
# Ordenar por similitud descendente, excluyendo el propio documento
ord <- order(sim[idx, ], decreasing = TRUE)
ord <- ord[ord != idx]
if (length(ord) == 0) {
return(tibble(titulo_recomendado = character(0), score = numeric(0)))
}
k <- min(k, length(ord))
tibble(
titulo_recomendado = titles_vec[ord[seq_len(k)]],
score = as.numeric(sim[idx, ord[seq_len(k)]])
)
}
#' Listar títulos disponibles (útil para autocompletar/depurar)
listar_titulos <- function(n = 20) {
tibble(idx = seq_along(titles), titulo = titles) %>%
slice_head(n = n)
}
# -------------------------------------------------------------
# 4) Ejemplos de uso (comenta/ajusta según tus títulos)
# -------------------------------------------------------------
# Ver algunos títulos disponibles
print(listar_titulos(15))
# Ejemplo (cambia por un título real de tu colección):
# resultados <- recomendador_tfidf("Messi es un perro", k = 10)
# print(resultados)
# Ejemplo (cambia por un título real de tu colección):
resultados <- recomendador_tfidf("Messi es un perro", k = 10)
print(resultados)
listar_titulos()
resultados <- recomendador_tfidf(20, k = 10)
print(resultados)
# =============================================================
# Pipeline: este archivo es la primera parte de proyecto de
#            desarrollo para un recomendador de cuentos del
#            escritor Hernán Casciari. Aquí desarrollamos el
#            pipeline de los datos, obteniéndolos de la DB
#            para su procesamiento correcto en pos de emplearlos
#            como insumo para el recomendador.
#
# 1) Buenas prácticas, librerías y descarga de datos
# 2) Análisis cuentos pre-procesamiento
# 3) Web scrapping stopwords argentinas
# 4) Procesamiento cuentos a tokens
#
# =============================================================
# =============================================================
# 1) Buenas prácticas, librerías y descarga de datos
# =============================================================
# Buenas prácticas
rm(list = ls())
#setwd("~/Desktop/Taller 1 - BigData")
# Librerías
require(pacman)
p_load(tidyverse, stringr, dplyr, stringi, tm, stopwords, tokenizers) #TODO para procesar data
p_load(rvest, udpipe) # para web scrapping
# Datos y primera visualización
db <- read.csv("blog_casciari.csv")
head(db)
# =============================================================
# 2) Análisis  cuentos pre-procesamiento
# =============================================================
# Ver final de cada cuento (últimas 5)
sapply(str_split(db$cuento, "\\s+"), function(x) paste(tail(x, 5), collapse = " "))
# Ver distribución tamaño string
# TODO ponerlo más lindo
db %>%
mutate(largo = nchar(cuento)) %>%
ggplot(aes(x = largo)) +
geom_histogram(bins = 30) +
labs(title = "Distribución del tamaño de los cuentos (caracteres)",
x = "Número de caracteres", y = "Frecuencia")
# Cambiar formato fecha
db <- db %>%
mutate(fecha = as.Date(fecha, format = "%m/%d/%y"))
# Ver distribución por fecha
db %>%
ggplot(aes(x = fecha)) +
geom_histogram(binwidth = 30, fill = "orange", color = "black") +
labs(title = "Distribución de cuentos por fecha",
x = "Fecha", y = "Número de cuentos")
# Son todos cuentos independientes? Hoy hay secuelas o continuaciones?
# =============================================================
# Web scrapping stopwords argentinas
# =============================================================
# Leer la página web con manejo de errores
tryCatch({
url <- "https://www.lifeder.com/frases-palabras-argentinas/"
# Si esta URL falla, podrías usar otra fuente de expresiones argentinas:
# url <- "https://languagetool.org/insights/es/publicacion/palabras-argentinas/"
page <- read_html(url)
# Extraer los nodos que contienen las palabras/expresiones
palabras_raw <- page %>%
html_nodes("strong, b") %>%      # selecciona etiquetas strong o b
html_text() %>%
str_trim()
# Filtrar y procesar; minuscula, duplicados, puntuación,
palabras_filt <- palabras_raw %>%
tolower() %>%
unique() %>%
# quitar puntuación
str_replace_all("[[:punct:]]", "") %>%
.[. != ""]  # eliminar strings vacíos
cat("Palabras argentinas extraídas:", length(palabras_filt), "\n")
}, error = function(e) {
cat("Error en web scraping, usando lista predefinida\n")
# Lista de backup con expresiones argentinas comunes
palabras_filt <- c("che", "boludo", "pibe", "mina", "laburo", "guita", "quilombo",
"bondi", "fiaca", "chabon", "mango", "faso", "morfi", "birra")
})
# Lematización usando udpipe
# Descargar y cargar modelo español (verificar si ya existe)
if(!file.exists("spanish-gsd-ud-2.5-191206.udpipe")) {
cat("Descargando modelo udpipe español...\n")
ud_model_file <- udpipe_download_model(language = "spanish")
ud_model <- udpipe_load_model(ud_model_file$file_model)
} else {
cat("Cargando modelo udpipe existente...\n")
ud_model <- udpipe_load_model("spanish-gsd-ud-2.5-191206.udpipe")
}
# Crear data frame para lematizar
df_arg <- data.frame(token = palabras_filt, stringsAsFactors = FALSE)
anot_arg <- udpipe_annotate(ud_model, x = df_arg$token)
anot_arg <- as.data.frame(anot_arg)
# Tomar el lemma de cada token
palabras_lemma <- anot_arg %>%
select(token, lemma) %>%
distinct() %>%
mutate(lemma = ifelse(is.na(lemma) | lemma == "", token, lemma)) %>%
pull(lemma) %>%
unique()
# Resultado final en una lista
stopwords_arg <- palabras_lemma
# Ver primeras 20
head(stopwords_arg, 20)
# =============================================================
# Procesamiento cuentos a tokens
# =============================================================
# Falta:
# - Lematización (revisar, no cambia mucho)
# - Quitar nombres (revisar importantes, como Chirri->amigo,Nina->hija, Mercedes->hogar/casa)
# - Quitar palabras argentinas (puede o no, depende si al final hay suficientes palabras)
#   - Web scapping de página (procesar minusculas, acentos y eso)
# - Preguntar Ignacio si cuentos muy largos afectan
# - Web scrapping para etiquetas y así sacar temas
#   - Esto podría llevar a enfoque k-means
# Eliminamos tildes y caracteres especiales del español
cuentos <- stri_trans_general(str = db$cuento, id = "Latin-ASCII")
# Quitar "/n" y reemplazar por espacio
cuentos <- str_replace_all(cuentos, "\n"," ")
# Quitar últimas 5 palabras, pues son nombre autor y fecha
cuentos <- sapply(str_split(cuentos, "\\s+"), function(x) paste(head(x, -5), collapse = " "))
# Volver strings en objetos tipo Corpus para funciones con la librería
cuentos <- Corpus(VectorSource(cuentos))
# Quitar puntuacion, números, espacios dobles, y pasar todo a minuscula
cuentos <- tm_map(cuentos,content_transformer(removePunctuation))
cuentos <- tm_map(cuentos,content_transformer(removeNumbers))
cuentos <- tm_map(cuentos,content_transformer(stripWhitespace))
cuentos <- tm_map(cuentos, content_transformer(tolower))
# Lista stopwords español de dos fuentes diferentes y combinamos
lista_palabras1 <- stopwords(language = "es", source = "snowball")
lista_palabras2 <- stopwords(language = "es", source = "nltk")
lista_palabras <- union(lista_palabras1, lista_palabras2)
# TODO: podría quitarse nombres o cosas recurrentes
# (Nombres buscar palabras empiezan mayúsculas y revisar) (revisar todos terminan con Hérnan Casciari)
# También lenguaje argento
# Agregamos palabras específica
lista_palabras <- c(lista_palabras,"mientras")
cuentos<-tm_map(cuentos, removeWords, lista_palabras)
# proximo miércoles hay actividad
# TODO cambiar esta parte
# Expresiones regulares para cambiar "miniseriedeTV" por "miniserie tv"
toTV <- content_transformer(function(x, pattern) {return (gsub(pattern, 'tv', x))})
# CORRECCIÓN DEL ERROR: Cambiar 'sinopsis' por 'cuentos'
cuentos <- tm_map(cuentos, toTV, "miniseriedetv")
cuentos <- tm_map(cuentos, toTV, "miniserie tv")
# Verificar contenido (ajustar índice si hay menos de 101 documentos)
if(length(cuentos) >= 101) {
cuentos[[101]]$content
} else {
cat("Menos de 101 documentos. Mostrando el último:\n")
print(cuentos[[length(cuentos)]]$content)
}
# Lematizar (modelo de udp ya descargado sección previa)
# Convertir corpus a data.frame temporal
texto_df <- data.frame(texto = sapply(cuentos, as.character), stringsAsFactors = FALSE)
cat("Procesando", nrow(texto_df), "documentos con udpipe...\n")
# Procesar con udpipe
anotaciones <- udpipe_annotate(ud_model, x = texto_df$texto)
anotaciones <- as.data.frame(anotaciones)
# Usar solo las lemas (en minúsculas)
cuentos_lematizados <- anotaciones %>%
group_by(doc_id) %>%
summarise(texto = paste(lemma, collapse = " "), .groups = "drop") %>%
pull(texto)
# Volver a objeto Corpus
cuentos <- Corpus(VectorSource(cuentos_lematizados))
# Convertimos el corpus lematizado a vector de texto
textos_lem <- sapply(cuentos, as.character)
cat("Generando n-gramas...\n")
# Generamos n-gramas de 1, 2 y 3 palabras
tokens_unigramas <- tokenize_words(textos_lem)
tokens_bigramas  <- tokenize_ngrams(textos_lem, n = 2)
tokens_trigramas <- tokenize_ngrams(textos_lem, n = 3)
# Combinamos todos los tokens en un solo texto por documento
tokens_combinados <- mapply(function(u, b, t) {
paste(c(u, b, t), collapse = " ")
}, tokens_unigramas, tokens_bigramas, tokens_trigramas)
# Volvemos al formato Corpus para continuar el pipeline
cuentos <- Corpus(VectorSource(tokens_combinados))
cat("Creando matriz documento-término...\n")
# Tokenizar (crear matriz documentos)
dtm_cuentos <- DocumentTermMatrix(cuentos)
cat("Dimensiones DTM original:", dim(dtm_cuentos), "\n")
# Quitar sparsity; quitar palabras que no están en 90% docs
dtm_cuentos <- removeSparseTerms(dtm_cuentos, sparse = 0.90)
db_raw <- read.csv("blog_casciari.csv", stringsAsFactors = FALSE)
# Asegura que #docs coincida
stopifnot(nrow(dtm_cuentos) == nrow(db_raw))
# Títulos únicos por si hay duplicados
titles <- make.unique(as.character(db_raw$titulo))
# Asigna los títulos como nombres de documento
dtm_cuentos$dimnames$Docs <- titles
# (opcional) guarda metadatos alineados
dir.create("stores", showWarnings = FALSE)
saveRDS(tibble(titulo = titles), "stores/meta_cuentos.rds")
cat("Dimensiones DTM después de sparse removal:", dim(dtm_cuentos), "\n")
# Crear directorio si no existe
dir.create("stores", showWarnings = FALSE)
# Exportar a stores
saveRDS(dtm_cuentos, file = "stores/dtm_cuentos.rds")
cat("Proceso completado. DTM guardada en stores/dtm_cuentos.rds\n")
cat("Documentos finales:", nrow(dtm_cuentos), "\n")
cat("Vocabulario final:", ncol(dtm_cuentos), "términos\n")
# =============================================================
# MOSTRAR MÉTRICAS EN PANTALLA
# Agregar al final de tu pipeline (después de crear dtm_cuentos)
# =============================================================
cat("\n===============================================\n")
cat("           MÉTRICAS DEL PIPELINE\n")
cat("===============================================\n")
# 1. Obtener DTM original para comparar
dtm_original <- DocumentTermMatrix(cuentos)
# 2. Extraer números clave
docs_finales <- nrow(dtm_cuentos)
terminos_finales <- ncol(dtm_cuentos)
terminos_originales <- ncol(dtm_original)
reduccion_absoluta <- terminos_originales - terminos_finales
reduccion_pct <- round((reduccion_absoluta / terminos_originales) * 100, 1)
# 3. Top términos
frecuencias <- colSums(as.matrix(dtm_cuentos))
top_terminos <- sort(frecuencias, decreasing = TRUE)
# 4. Sparsity real
sparsity_real <- round((1 - sum(dtm_cuentos > 0) / (docs_finales * terminos_finales)) * 100, 1)
# 5. MOSTRAR TABLA PRINCIPAL
cat("DIMENSIONES Y REDUCCIÓN:\n")
cat("========================\n")
cat("Documentos finales:              ", docs_finales, "\n")
cat("Términos originales:             ", terminos_originales, "\n")
cat("Términos finales:                ", terminos_finales, "\n")
cat("Términos eliminados:             ", reduccion_absoluta, "\n")
cat("Reducción vocabulario:           ", reduccion_pct, "%\n")
cat("Sparsity aplicada:               90%\n")
cat("Sparsity real resultante:        ", sparsity_real, "%\n")
# 6. MOSTRAR TOP TÉRMINOS
cat("\nTOP 10 TÉRMINOS MÁS FRECUENTES:\n")
cat("===============================\n")
for(i in 1:min(10, length(top_terminos))) {
cat(sprintf("%2d. %-15s (%3d docs)\n", i, names(top_terminos)[i], top_terminos[i]))
}
# 7. TEXTO LISTO PARA REPORTE
cat("\n===============================================\n")
cat("           TEXTO PARA TU REPORTE\n")
cat("===============================================\n")
texto_reporte <- sprintf(
"La matriz documento-término final presenta dimensiones de %d documentos × %d términos tras aplicar un umbral de sparsity del 90%%. El vocabulario se redujo de %d a %d términos únicos, representando una disminución del %s%% que optimiza la representación semántica. Los términos más frecuentes post-procesamiento son: %s.",
docs_finales,
terminos_finales,
terminos_originales,
terminos_finales,
reduccion_pct,
paste(names(top_terminos)[1:5], collapse = ", ")
)
cat(texto_reporte, "\n")
cat("\n===============================================\n")
# =============================================================
# Recomendador TF-IDF + Similitud del Coseno (robusto)
# =============================================================
# Limpieza y librerías
rm(list = ls())
require(pacman)
p_load(tidyverse, tm, proxy)
# -------------------------------------------------------------
# 1) Carga de datos
# -------------------------------------------------------------
# DTM lematizada con n-gramas que generaron en el pipeline
dtm_path  <- "stores/dtm_cuentos.rds"
meta_path <- "stores/meta_cuentos.rds"  # opcional (doc_id, titulo)
if (!file.exists(dtm_path)) {
stop("No se encontró 'stores/dtm_cuentos.rds'. Ejecuta primero el pipeline.")
}
dtm <- readRDS(dtm_path)
# Cargar metadatos si existen
meta <- NULL
if (file.exists(meta_path)) {
meta <- readRDS(meta_path)
# Se espera que meta tenga una columna 'titulo' alineada con el orden de filas de la DTM
if (!is.data.frame(meta) || is.null(meta$titulo)) {
warning("El archivo 'meta_cuentos.rds' no tiene columna 'titulo'. Se ignorará meta.")
meta <- NULL
} else if (nrow(meta) != nrow(dtm)) {
warning("El número de filas de meta no coincide con la DTM. Se ignorará meta.")
meta <- NULL
}
}
# Derivar títulos: prioridad meta$titulo > Docs(dtm) > títulos de respaldo
titles <- NULL
if (!is.null(meta)) {
titles <- as.character(meta$titulo)
} else {
# tm::Docs extrae los IDs/nombres de documento si existen
titles <- tryCatch(Docs(dtm), error = function(e) NULL)
if (is.null(titles) || any(is.na(titles))) {
titles <- rownames(as.matrix(dtm))
}
if (is.null(titles) || any(is.na(titles)) || length(titles) != nrow(dtm)) {
# respaldo seguro
titles <- sprintf("doc_%04d", seq_len(nrow(dtm)))
}
}
# -------------------------------------------------------------
# 2) Construcción matriz TF-IDF normalizada y similitud coseno
# -------------------------------------------------------------
# Ponderar por TF-IDF
tfidf <- weightTfIdf(dtm)
M <- as.matrix(tfidf)
# Manejo de filas con norma cero (documentos vacíos tras preprocesamiento)
row_norm <- sqrt(rowSums(M^2))
keep <- row_norm > 0
if (!all(keep)) {
message(sprintf("Se eliminaron %d documentos con vector TF-IDF nulo.", sum(!keep)))
}
M <- M[keep, , drop = FALSE]
row_norm <- row_norm[keep]
M <- M / row_norm
titles <- titles[keep]
# Similitud de coseno (proxy::simil devuelve similitud directamente)
S <- proxy::simil(M, M, method = "cosine")
S <- as.matrix(S)  # para indexación conveniente
# -------------------------------------------------------------
# 3) Funciones de recomendación
# -------------------------------------------------------------
#' Recomendador basado en TF-IDF + coseno
#' @param title Título exacto del cuento de consulta
#' @param k número de recomendaciones a devolver
#' @return tibble con titulo_recomendado y score (similitud coseno)
recomendador_tfidf <- function(title, k = 10, titles_vec = titles, sim = S) {
stopifnot(length(title) == 1L)
idx <- match(title, titles_vec)
if (is.na(idx)) {
stop("Título no encontrado. Usa 'listar_titulos()' para ver disponibles.")
}
# Ordenar por similitud descendente, excluyendo el propio documento
ord <- order(sim[idx, ], decreasing = TRUE)
ord <- ord[ord != idx]
if (length(ord) == 0) {
return(tibble(titulo_recomendado = character(0), score = numeric(0)))
}
k <- min(k, length(ord))
tibble(
titulo_recomendado = titles_vec[ord[seq_len(k)]],
score = as.numeric(sim[idx, ord[seq_len(k)]])
)
}
#' Listar títulos disponibles (útil para autocompletar/depurar)
listar_titulos <- function(n = 20) {
tibble(idx = seq_along(titles), titulo = titles) %>%
slice_head(n = n)
}
# -------------------------------------------------------------
# 4) Ejemplos de uso (comenta/ajusta según tus títulos)
# -------------------------------------------------------------
# Ver algunos títulos disponibles
print(listar_titulos(15))
# Ejemplo (cambia por un título real de tu colección):
#resultados <- recomendador_tfidf(20, k = 10)
#print(resultados)
