#     + stopwords normalizadas + argentinismos
# -------------------------------------------------------------
# Lista stopwords español de dos fuentes diferentes y combinamos
lista_palabras1 <- stopwords(language = "es", source = "snowball")
lista_palabras2 <- stopwords(language = "es", source = "nltk")
lista_palabras <- union(lista_palabras1, lista_palabras2)
cat("Procesando", nrow(texto_df), "documentos con udpipe...\n")
anotaciones <- udpipe_annotate(ud_model, x = texto_df$texto)
anotaciones <- as.data.frame(anotaciones)
# Filtra categorías poco informativas
filtro_upos <- c("PRON","DET","AUX","PART","ADP","CCONJ","SCONJ","PUNCT","SYM","NUM","X")
anotaciones <- anotaciones %>%
filter(!(upos %in% filtro_upos))
# Usa lemma si existe; si no, token
anotaciones <- anotaciones %>%
mutate(lemma = ifelse(is.na(lemma) | lemma == "", token, lemma))
# Normaliza lemmas (sin tildes, minúsculas, limpio)
anotaciones$lemma <- normalize_plain(anotaciones$lemma)
# Stopwords (unión de dos fuentes) normalizadas
lista_palabras1 <- stopwords(language = "es", source = "snowball")
lista_palabras2 <- stopwords(language = "es", source = "nltk")
lista_palabras <- union(lista_palabras1, lista_palabras2)
lista_palabras <- normalize_plain(lista_palabras)
# Añade propias/funcionales que puedan colarse
lista_palabras <- unique(c(
lista_palabras, "el", "la", "los", "las", "decir", "hacer", "tener"
, "vez", "ver"
))
# También quita argentinismos si quieres neutralizarlos
lista_stop_extra <- unique(c(lista_palabras))
# Aplica stopwords y borra vacíos
anotaciones <- anotaciones %>%
filter(!(lemma %in% lista_stop_extra),
lemma != "")
# Reconstruye textos lematizados y normalizados por doc
cuentos_lematizados <- anotaciones %>%
group_by(doc_id) %>%
summarise(texto = paste(lemma, collapse = " "), .groups = "drop") %>%
pull(texto)
# -------------------------------------------------------------
# 7) N-gramas (1-3) a partir de textos ya normalizados
# -------------------------------------------------------------
cat("Generando n-gramas...\n")
tokens_unigramas <- tokenize_words(cuentos_lematizados)
tokens_bigramas  <- tokenize_ngrams(cuentos_lematizados, n = 2)
tokens_trigramas <- tokenize_ngrams(cuentos_lematizados, n = 3)
tokens_combinados <- mapply(function(u, b, t) {
paste(c(u, b, t), collapse = " ")
}, tokens_unigramas, tokens_bigramas, tokens_trigramas)
# Corpus final (normalizado, sin tildes, con n-gramas)
cuentos <- Corpus(VectorSource(tokens_combinados))
cuentos<-tm_map(cuentos, removeWords, lista_palabras)
# Quitar puntuacion, números, espacios dobles, y pasar todo a minuscula
cuentos <- tm_map(cuentos,content_transformer(removePunctuation))
cuentos <- tm_map(cuentos,content_transformer(removeNumbers))
cuentos <- tm_map(cuentos,content_transformer(stripWhitespace))
cuentos <- tm_map(cuentos, content_transformer(tolower))
# Definir transformador general para reemplazar texto
toWord <- content_transformer(function(x, pattern, replacement) {
gsub(pattern, replacement, x, ignore.case = TRUE)
})
# Aplicar transformaciones con expresiones regulares
cuentos <- tm_map(cuentos, toWord, "\\bChirri\\b", "amigo")
cuentos <- tm_map(cuentos, toWord, "\\bNina\\b", "hija")
cuentos <- tm_map(cuentos, toWord, "\\bMercedes\\b", "hogar")
cuentos <- tm_map(cuentos, toWord, "\\bTotin\\b", "perro")
cuentos <- tm_map(cuentos, toWord, "\\bCristina\\b", "esposa")
# Limpieza adicional de casos específicos (ejemplo miniserie de TV)
toTV <- content_transformer(function(x, pattern) gsub(pattern, "tv", x))
cuentos <- tm_map(cuentos, toTV, "miniseriedetv")
cuentos <- tm_map(cuentos, toTV, "miniserie tv")
# (Opcional) inspección
if(length(cuentos) >= 1) {
cat("Ejemplo documento 1 (1000 chars):\n",
substr(cuentos[[1]]$content, 1, 1000), "\n\n")
}
# -------------------------------------------------------------
# 8) Matriz Documento-Término (DTM) y sparsity
# -------------------------------------------------------------
cat("Creando matriz documento-término...\n")
dtm_cuentos <- DocumentTermMatrix(cuentos)
cat("Dimensiones DTM original:", paste(dim(dtm_cuentos), collapse = " x "), "\n")
# Quitar términos muy escasos (sparsity <= 0.90 -> presentes ≥10% docs)
dtm_cuentos <- removeSparseTerms(dtm_cuentos, sparse = 0.90)
# Releer original para metadata de títulos
db_raw <- read.csv("blog_casciari.csv", stringsAsFactors = FALSE)
# Asegura que #docs coincida
stopifnot(nrow(dtm_cuentos) == nrow(db_raw))
# Títulos únicos para docnames
titles <- make.unique(as.character(db_raw$titulo))
dtm_cuentos$dimnames$Docs <- titles
# Guarda metadatos
dir.create("stores", showWarnings = FALSE)
saveRDS(tibble(titulo = titles), "stores/meta_cuentos.rds")
cat("Dimensiones DTM después de sparse removal:",
paste(dim(dtm_cuentos), collapse = " x "), "\n")
# Exporta DTM
saveRDS(dtm_cuentos, file = "stores/dtm_cuentos.rds")
cat("Proceso completado. DTM guardada en stores/dtm_cuentos.rds\n")
# -------------------------------------------------------------
# 9) Métricas y resumen para reporte
# -------------------------------------------------------------
cat("\n===============================================\n")
cat("           MÉTRICAS DEL PIPELINE\n")
cat("===============================================\n")
# DTM original (sobre el mismo 'cuentos' final sin sparsity)
dtm_original <- DocumentTermMatrix(cuentos)
docs_finales        <- nrow(dtm_cuentos)
terminos_finales     <- ncol(dtm_cuentos)
terminos_originales  <- ncol(dtm_original)
reduccion_absoluta   <- terminos_originales - terminos_finales
reduccion_pct        <- round((reduccion_absoluta / terminos_originales) * 100, 1)
# Frecuencias de términos en la DTM final
frecuencias <- colSums(as.matrix(dtm_cuentos))
top_terminos <- sort(frecuencias, decreasing = TRUE)
# Sparsity real (porcentaje de ceros)
mat_bin <- as.matrix(dtm_cuentos) > 0
sparsity_real <- round((1 - sum(mat_bin) / (docs_finales * terminos_finales)) * 100, 1)
cat("DIMENSIONES Y REDUCCIÓN:\n")
cat("========================\n")
cat("Documentos finales:              ", docs_finales, "\n")
cat("Términos originales:             ", terminos_originales, "\n")
cat("Términos finales:                ", terminos_finales, "\n")
cat("Términos eliminados:             ", reduccion_absoluta, "\n")
cat("Reducción vocabulario:           ", reduccion_pct, "%\n")
cat("Sparsity aplicada:               90%\n")
cat("Sparsity real resultante:        ", sparsity_real, "%\n")
cat("\nTOP 10 TÉRMINOS MÁS FRECUENTES:\n")
cat("===============================\n")
n_top <- min(10, length(top_terminos))
for(i in 1:n_top) {
cat(sprintf("%2d. %-20s %8d\n", i, names(top_terminos)[i], top_terminos[i]))
}
cat("\n===============================================\n")
cat("           TEXTO PARA TU REPORTE\n")
cat("===============================================\n")
texto_reporte <- sprintf(
"La matriz documento-término final presenta dimensiones de %d documentos × %d términos tras aplicar un umbral de sparsity del 90%%. El vocabulario se redujo de %d a %d términos únicos (−%s%%). Los términos más frecuentes post-procesamiento son: %s.",
docs_finales,
terminos_finales,
terminos_originales,
terminos_finales,
reduccion_pct,
paste(names(top_terminos)[1:min(5, length(top_terminos))], collapse = ", ")
)
cat(texto_reporte, "\n")
cat("\n===============================================\n")
# =============================================================
# Recomendadores: este archivo es la segunda parte de proyecto.
#            Ahora se desarrollan 2 metodologías de recomendación
#            de cuentos del escritor Hernán Casciari. Primero se
#            implementa uno basado en TF-IDF y similitud del coseno,
#            luego uno basado en LDA.
#
# 1) Buenas prácticas, librerías y descarga de datos
# 2) Recomendador TF-IDF y similitud del coseno
# 3) Recomendador LDA
#   3.1) Matrices de probabilidades
#   3.2) Probabilidades documento-tópico
#   3.3) Distribución de probabilidades γ por tópico
#   3.4) Matriz documento-tópico y similitud
#   3.5) Función recomendadora LDA
# =============================================================
# =============================================================
# 1) Buenas prácticas, librerías y descarga de datos
# =============================================================
# Buenas prácticas
#rm(list = ls())
# NOTA: CAMBIAR DIRECTORIO
# setwd("C:/Users/Asuar/OneDrive/Escritorio/Libros Clases/Economía/Ciencia Datos y Econometria/Taller3-blog_casciari")
# Librerías
require(pacman)
p_load(tidyverse, stringr, dplyr, tm, proxy, maptpx, tidytext) #TODO para procesar data
# Datos y primera visualización
cuentos <- readRDS("stores/dtm_cuentos.rds") # cuentos procesados
db <- read.csv("blog_casciari.csv") # db original para cuentos
# A los cuentos origionales quitar salto página; no aporta nada
# Quitar "/n" y reemplazar por espacio
db$cuento <- str_replace_all(db$cuento, "\n"," ")
# =============================================================
# 2) Recomendador TF-IDF y similitud del coseno
# =============================================================
# Crear matriz TF-IDF a partir del corpus de cuentos
tfidf <- weightTfIdf(cuentos)
# Matriz TF-IDF
tfidf_matrix <- as.matrix(tfidf)
# TODO Se podría normalizar los vectores antes de similitud para controlar efecto por longitud
tfidf_matrix <- tfidf_matrix / sqrt(rowSums(tfidf_matrix^2))
# Calcular similitud de coseno entre documentos
cosine_sim <- 1 - proxy::dist(tfidf_matrix, tfidf_matrix, method = "cosine")
# TODO Revisar un ejemplo rápido (opcional)
cosine_sim[1, 1:10]
# Asegurar que 'db' tenga una columna de índice (para vincular títulos y filas del DTM)
db$index <- 1:nrow(db)
# Crear función recomendadora
# TODO revisar que orden en dtm sea el mismo que en db
recomendador_tfidf <- function(title, distancias = cosine_sim, df = db) {
# Obtener el índice del cuento
idx <- df$index[df$titulo == title]
if (length(idx) == 0) {
stop("El título ingresado no existe en la base de datos.")
}
# Crear un data.frame con las distancias y los títulos
recomendaciones <- data.frame(
dist = as.numeric(distancias[idx, ]),
titulo = df$titulo
)
# Ordenar de mayor a menor similitud
recomendaciones <- recomendaciones %>% arrange(desc(dist))
# Retornar los 10 cuentos más similares (excluyendo el mismo)
return(recomendaciones$titulo[1:11])
}
# Ejemplo de uso
recomendador_tfidf("Messi es un perro")
# Revisar recomendación (para "Messi es un perro" habla de infancia y periodismo)
c1 <- db %>% filter(titulo == "Messi es un perro") %>% pull(cuento)
c2 <- db %>% filter(titulo == "Matar la crisis a volantazos") %>% pull(cuento)
c1
c2
# =============================================================
# 3) Recomendador LDA
# =============================================================
# Definir valores de K a probar
grid <- c(3, 5, 7, 10)
# Crear lista vacía para guardar los modelos
modelos_lda <- list() # Se guardarán estilo "K3, K5, K7, K10"
# Fijar semilla
set.seed(123)
# Entrenar modelos y guardarlos en la lista
for (k in grid) {
cat("Entrenando modelo con K =", k, "...\n")
modelo <- topics(cuentos, K = k)
modelos_lda[[paste0("K", k)]] <- modelo
cat("Modelo con", k, "tópicos completado.\n\n")
}
Kis <- list(K3, K5, K7, K10)
modelos <- list(modelos_lda$K3, modelos_lda$K5, modelos_lda$K7, modelos_lda$K10)
for (modelo in modelos){
# TODO: cambiar el # de la "K" para cambiar el modelo
cuentos_lda <- modelo
# =============================================================
# 3.1) Matrices de probabilidades
# =============================================================
# En maptpx:
#  - theta: probabilidad palabra-tópico
#  - omega: probabilidad documento-tópico
# Extraer theta (palabra-tópico)
theta_matrix <- cuentos_lda$theta
# Convertir a formato tidy
cuentos_topics <- as.data.frame(theta_matrix) %>%
mutate(term = rownames(theta_matrix)) %>%
pivot_longer(cols = -term,
names_to = "topic",
values_to = "beta") %>%
mutate(topic = as.numeric(gsub("V", "", topic)))
# Obtener los 10 términos más representativos por tópico
cuentos_top_terms <- cuentos_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
# Visualización de tópicos
cuentos_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered() +
labs(title = "Top 10 términos por tópico",
x = "Beta (probabilidad palabra-tópico)",
y = "Término")
# =============================================================
# 3.2) Probabilidades documento-tópico
# =============================================================
# Extraer omega
omega_matrix <- cuentos_lda$omega
cuentos_documents <- as.data.frame(omega_matrix) %>%
mutate(document = row_number()) %>%
pivot_longer(cols = -document,
names_to = "topic",
values_to = "gamma") %>%
mutate(topic = as.numeric(gsub("V", "", topic)))
# =============================================================
# 3.3) Distribución de probabilidades γ por tópico
# =============================================================
cuentos_documents %>%
ggplot(aes(x = factor(topic), y = gamma, fill = factor(topic))) +
geom_boxplot() +
labs(title = "Distribución de probabilidades γ por tópico",
x = "Tópico",
y = "Probabilidad documento-tópico",
fill = "Tópico")
# =============================================================
# 3.4) Matriz documento-tópico y similitud
# =============================================================
gamma_wide <- cuentos_documents %>%
pivot_wider(names_from = topic,
values_from = gamma,
names_prefix = "topic_")
gamma_matrix <- as.matrix(gamma_wide[, -1])
rownames(gamma_matrix) <- gamma_wide$document
# Calcular similitud de coseno entre documentos
cosine_similarity <- function(matrix) {
norm_matrix <- matrix / sqrt(rowSums(matrix^2))
sim <- norm_matrix %*% t(norm_matrix)
return(sim)
}
similarity_matrix <- cosine_similarity(gamma_matrix)
# =============================================================
# Función recomendadora LDA
# =============================================================
recomendador_lda <- function(title, distancias = similarity_matrix, df = db) {
# Obtener el índice del cuento
idx <- df$index[df$titulo == title]
if (length(idx) == 0) {
stop("El título ingresado no existe en la base de datos.")
}
# Crear un data.frame con las distancias y los títulos
recomendaciones <- data.frame(
dist = distancias[idx, ],
titulo = df$titulo
)
# Ordenar de mayor a menor similitud
recomendaciones <- recomendaciones %>% arrange(-dist)
# Retornar los 10 cuentos más similares (excluyendo el mismo)
return(recomendaciones$titulo[2:11])
}
# Ejemplo de uso
recomendador_lda("Messi es un perro")}
for (modelo in modelos){
# TODO: cambiar el # de la "K" para cambiar el modelo
cuentos_lda <- modelo
# =============================================================
# 3.1) Matrices de probabilidades
# =============================================================
# En maptpx:
#  - theta: probabilidad palabra-tópico
#  - omega: probabilidad documento-tópico
# Extraer theta (palabra-tópico)
theta_matrix <- cuentos_lda$theta
# Convertir a formato tidy
cuentos_topics <- as.data.frame(theta_matrix) %>%
mutate(term = rownames(theta_matrix)) %>%
pivot_longer(cols = -term,
names_to = "topic",
values_to = "beta") %>%
mutate(topic = as.numeric(gsub("V", "", topic)))
# Obtener los 10 términos más representativos por tópico
cuentos_top_terms <- cuentos_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
# Visualización de tópicos
cuentos_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered() +
labs(title = "Top 10 términos por tópico",
x = "Beta (probabilidad palabra-tópico)",
y = "Término")
# =============================================================
# 3.2) Probabilidades documento-tópico
# =============================================================
# Extraer omega
omega_matrix <- cuentos_lda$omega
cuentos_documents <- as.data.frame(omega_matrix) %>%
mutate(document = row_number()) %>%
pivot_longer(cols = -document,
names_to = "topic",
values_to = "gamma") %>%
mutate(topic = as.numeric(gsub("V", "", topic)))
# =============================================================
# 3.3) Distribución de probabilidades γ por tópico
# =============================================================
cuentos_documents %>%
ggplot(aes(x = factor(topic), y = gamma, fill = factor(topic))) +
geom_boxplot() +
labs(title = "Distribución de probabilidades γ por tópico",
x = "Tópico",
y = "Probabilidad documento-tópico",
fill = "Tópico")
# =============================================================
# 3.4) Matriz documento-tópico y similitud
# =============================================================
gamma_wide <- cuentos_documents %>%
pivot_wider(names_from = topic,
values_from = gamma,
names_prefix = "topic_")
gamma_matrix <- as.matrix(gamma_wide[, -1])
rownames(gamma_matrix) <- gamma_wide$document
# Calcular similitud de coseno entre documentos
cosine_similarity <- function(matrix) {
norm_matrix <- matrix / sqrt(rowSums(matrix^2))
sim <- norm_matrix %*% t(norm_matrix)
return(sim)
}
similarity_matrix <- cosine_similarity(gamma_matrix)
# =============================================================
# Función recomendadora LDA
# =============================================================
recomendador_lda <- function(title, distancias = similarity_matrix, df = db) {
# Obtener el índice del cuento
idx <- df$index[df$titulo == title]
if (length(idx) == 0) {
stop("El título ingresado no existe en la base de datos.")
}
# Crear un data.frame con las distancias y los títulos
recomendaciones <- data.frame(
dist = distancias[idx, ],
titulo = df$titulo
)
# Ordenar de mayor a menor similitud
recomendaciones <- recomendaciones %>% arrange(-dist)
# Retornar los 10 cuentos más similares (excluyendo el mismo)
return(recomendaciones$titulo[2:11])
}
# Ejemplo de uso
recomendador_lda("Messi es un perro")}
# TODO: cambiar el # de la "K" para cambiar el modelo
cuentos_lda <- modelos_lda$K3
# =============================================================
# 3.1) Matrices de probabilidades
# =============================================================
# En maptpx:
#  - theta: probabilidad palabra-tópico
#  - omega: probabilidad documento-tópico
# Extraer theta (palabra-tópico)
theta_matrix <- cuentos_lda$theta
# Convertir a formato tidy
cuentos_topics <- as.data.frame(theta_matrix) %>%
mutate(term = rownames(theta_matrix)) %>%
pivot_longer(cols = -term,
names_to = "topic",
values_to = "beta") %>%
mutate(topic = as.numeric(gsub("V", "", topic)))
# Obtener los 10 términos más representativos por tópico
cuentos_top_terms <- cuentos_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
# Visualización de tópicos
cuentos_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered() +
labs(title = "Top 10 términos por tópico",
x = "Beta (probabilidad palabra-tópico)",
y = "Término")
# =============================================================
# 3.2) Probabilidades documento-tópico
# =============================================================
# Extraer omega
omega_matrix <- cuentos_lda$omega
cuentos_documents <- as.data.frame(omega_matrix) %>%
mutate(document = row_number()) %>%
pivot_longer(cols = -document,
names_to = "topic",
values_to = "gamma") %>%
mutate(topic = as.numeric(gsub("V", "", topic)))
# =============================================================
# 3.3) Distribución de probabilidades γ por tópico
# =============================================================
cuentos_documents %>%
ggplot(aes(x = factor(topic), y = gamma, fill = factor(topic))) +
geom_boxplot() +
labs(title = "Distribución de probabilidades γ por tópico",
x = "Tópico",
y = "Probabilidad documento-tópico",
fill = "Tópico")
# =============================================================
# 3.4) Matriz documento-tópico y similitud
# =============================================================
gamma_wide <- cuentos_documents %>%
pivot_wider(names_from = topic,
values_from = gamma,
names_prefix = "topic_")
gamma_matrix <- as.matrix(gamma_wide[, -1])
rownames(gamma_matrix) <- gamma_wide$document
# Calcular similitud de coseno entre documentos
cosine_similarity <- function(matrix) {
norm_matrix <- matrix / sqrt(rowSums(matrix^2))
sim <- norm_matrix %*% t(norm_matrix)
return(sim)
}
similarity_matrix <- cosine_similarity(gamma_matrix)
# =============================================================
# Función recomendadora LDA
# =============================================================
recomendador_lda <- function(title, distancias = similarity_matrix, df = db) {
# Obtener el índice del cuento
idx <- df$index[df$titulo == title]
if (length(idx) == 0) {
stop("El título ingresado no existe en la base de datos.")
}
# Crear un data.frame con las distancias y los títulos
recomendaciones <- data.frame(
dist = distancias[idx, ],
titulo = df$titulo
)
# Ordenar de mayor a menor similitud
recomendaciones <- recomendaciones %>% arrange(-dist)
# Retornar los 10 cuentos más similares (excluyendo el mismo)
return(recomendaciones$titulo[2:11])
}
# Ejemplo de uso
recomendador_lda("Messi es un perro")
