# 2) Recomendador TF-IDF y similitud del coseno
# =============================================================
# Crear matriz TF-IDF a partir del corpus de cuentos
tfidf <- weightTfIdf(db)
# Matriz TF-IDF
tfidf_matrix <- as.matrix(tfidf)
# TODO Se podría normalizar los vectores antes de similitud para controlar efecto por longitud
tfidf_matrix <- tfidf_matrix / sqrt(rowSums(tfidf_matrix^2))
# Calcular similitud de coseno entre documentos
cosine_sim <- 1 - proxy::dist(tfidf_matrix, tfidf_matrix, method = "cosine")
# TODO Revisar un ejemplo rápido (opcional)
cosine_sim[1, 1:10]
# Crear función recomendadora
# TODO revisar que orden en dtm sea el mismo que en db
recomendador_tfidf <- function(title, distancias = cosine_sim, df = db) {
# Obtener el índice del cuento
idx <- df$index[df$titulo == title]
if (length(idx) == 0) {
stop("El título ingresado no existe en la base de datos.")
}
# Crear un data.frame con las distancias y los títulos
recomendaciones <- data.frame(
dist = distancias[idx, ],
titulo = df$titulo
)
# Ordenar de mayor a menor similitud
recomendaciones <- recomendaciones %>% arrange(-dist)
# Retornar los 10 cuentos más similares (excluyendo el mismo)
return(recomendaciones$titulo[2:11])
}
# Ejemplo de uso
recomendador_tfidf("Messi es un perro")
# =============================================================
# Pipeline: este archivo es la primera parte de proyecto de
#            desarrollo para un recomendador de cuentos del
#            escritor Hernán Casciari. Aquí desarrollamos el
#            pipeline de los datos, obteniéndolos de la DB
#            para su procesamiento correcto en pos de emplearlos
#            como insumo para el recomendador.
#
# 1) Buenas prácticas, librerías y descarga de datos
# 2) Análisis cuentos pre-procesamiento
# 3) Web scrapping stopwords argentinas
# 4) Procesamiento cuentos a tokens
#
# =============================================================
# =============================================================
# 1) Buenas prácticas, librerías y descarga de datos
# =============================================================
# Buenas prácticas
rm(list = ls())
setwd("~/Desktop/Taller 1 - BigData")
setwd("~/Desktop/andes/12do semestre/Ciencia de datos y econometría aplicada/Taller3-blog_casciari")
# =============================================================
# Pipeline: este archivo es la primera parte de proyecto de
#            desarrollo para un recomendador de cuentos del
#            escritor Hernán Casciari. Aquí desarrollamos el
#            pipeline de los datos, obteniéndolos de la DB
#            para su procesamiento correcto en pos de emplearlos
#            como insumo para el recomendador.
#
# 1) Buenas prácticas, librerías y descarga de datos
# 2) Análisis cuentos pre-procesamiento
# 3) Web scrapping stopwords argentinas
# 4) Procesamiento cuentos a tokens
#
# =============================================================
# =============================================================
# 1) Buenas prácticas, librerías y descarga de datos
# =============================================================
# Buenas prácticas
rm(list = ls())
#setwd("~/Desktop/Taller 1 - BigData")
# Librerías
require(pacman)
p_load(tidyverse, stringr, dplyr, stringi, tm, stopwords, tokenizers) #TODO para procesar data
p_load(rvest, udpipe) # para web scrapping
# Datos y primera visualización
db <- read.csv("blog_casciari.csv")
setwd("~/Desktop/andes/12do semestre/Ciencia de datos y econometría aplicada/Taller3-blog_casciari/stores")
# =============================================================
# Pipeline: este archivo es la primera parte de proyecto de
#            desarrollo para un recomendador de cuentos del
#            escritor Hernán Casciari. Aquí desarrollamos el
#            pipeline de los datos, obteniéndolos de la DB
#            para su procesamiento correcto en pos de emplearlos
#            como insumo para el recomendador.
#
# 1) Buenas prácticas, librerías y descarga de datos
# 2) Análisis cuentos pre-procesamiento
# 3) Web scrapping stopwords argentinas
# 4) Procesamiento cuentos a tokens
#
# =============================================================
# =============================================================
# 1) Buenas prácticas, librerías y descarga de datos
# =============================================================
# Buenas prácticas
rm(list = ls())
#setwd("~/Desktop/Taller 1 - BigData")
# Librerías
require(pacman)
p_load(tidyverse, stringr, dplyr, stringi, tm, stopwords, tokenizers) #TODO para procesar data
p_load(rvest, udpipe) # para web scrapping
# Datos y primera visualización
db <- read.csv("blog_casciari.csv")
head(db)
# =============================================================
# 2) Análisis  cuentos pre-procesamiento
# =============================================================
# Ver final de cada cuento (últimas 5)
sapply(str_split(db$cuento, "\\s+"), function(x) paste(tail(x, 5), collapse = " "))
# Ver distribución tamaño string
# TODO ponerlo más lindo
db %>%
mutate(largo = nchar(cuento)) %>%
ggplot(aes(x = largo)) +
geom_histogram(bins = 30) +
labs(title = "Distribución del tamaño de los cuentos (caracteres)",
x = "Número de caracteres", y = "Frecuencia")
# Cambiar formato fecha
db <- db %>%
mutate(fecha = as.Date(fecha, format = "%m/%d/%y"))
# Ver distribución por fecha
db %>%
ggplot(aes(x = fecha)) +
geom_histogram(binwidth = 30, fill = "orange", color = "black") +
labs(title = "Distribución de cuentos por fecha",
x = "Fecha", y = "Número de cuentos")
# Son todos cuentos independientes? Hoy hay secuelas o continuaciones?
# =============================================================
# Web scrapping stopwords argentinas
# =============================================================
# Leer la página web con manejo de errores
tryCatch({
url <- "https://www.lifeder.com/frases-palabras-argentinas/"
# Si esta URL falla, podrías usar otra fuente de expresiones argentinas:
# url <- "https://languagetool.org/insights/es/publicacion/palabras-argentinas/"
page <- read_html(url)
# Extraer los nodos que contienen las palabras/expresiones
palabras_raw <- page %>%
html_nodes("strong, b") %>%      # selecciona etiquetas strong o b
html_text() %>%
str_trim()
# Filtrar y procesar; minuscula, duplicados, puntuación,
palabras_filt <- palabras_raw %>%
tolower() %>%
unique() %>%
# quitar puntuación
str_replace_all("[[:punct:]]", "") %>%
.[. != ""]  # eliminar strings vacíos
cat("Palabras argentinas extraídas:", length(palabras_filt), "\n")
}, error = function(e) {
cat("Error en web scraping, usando lista predefinida\n")
# Lista de backup con expresiones argentinas comunes
palabras_filt <- c("che", "boludo", "pibe", "mina", "laburo", "guita", "quilombo",
"bondi", "fiaca", "chabon", "mango", "faso", "morfi", "birra")
})
# Lematización usando udpipe
# Descargar y cargar modelo español (verificar si ya existe)
if(!file.exists("spanish-gsd-ud-2.5-191206.udpipe")) {
cat("Descargando modelo udpipe español...\n")
ud_model_file <- udpipe_download_model(language = "spanish")
ud_model <- udpipe_load_model(ud_model_file$file_model)
} else {
cat("Cargando modelo udpipe existente...\n")
ud_model <- udpipe_load_model("spanish-gsd-ud-2.5-191206.udpipe")
}
# Crear data frame para lematizar
df_arg <- data.frame(token = palabras_filt, stringsAsFactors = FALSE)
anot_arg <- udpipe_annotate(ud_model, x = df_arg$token)
anot_arg <- as.data.frame(anot_arg)
# Tomar el lemma de cada token
palabras_lemma <- anot_arg %>%
select(token, lemma) %>%
distinct() %>%
mutate(lemma = ifelse(is.na(lemma) | lemma == "", token, lemma)) %>%
pull(lemma) %>%
unique()
# Resultado final en una lista
stopwords_arg <- palabras_lemma
# Ver primeras 20
head(stopwords_arg, 20)
# =============================================================
# Procesamiento cuentos a tokens
# =============================================================
# Falta:
# - Lematización (revisar, no cambia mucho)
# - Quitar nombres (revisar importantes, como Chirri->amigo,Nina->hija, Mercedes->hogar/casa)
# - Quitar palabras argentinas (puede o no, depende si al final hay suficientes palabras)
#   - Web scapping de página (procesar minusculas, acentos y eso)
# - Preguntar Ignacio si cuentos muy largos afectan
# - Web scrapping para etiquetas y así sacar temas
#   - Esto podría llevar a enfoque k-means
# Eliminamos tildes y caracteres especiales del español
cuentos <- stri_trans_general(str = db$cuento, id = "Latin-ASCII")
# Quitar "/n" y reemplazar por espacio
cuentos <- str_replace_all(cuentos, "\n"," ")
# Quitar últimas 5 palabras, pues son nombre autor y fecha
cuentos <- sapply(str_split(cuentos, "\\s+"), function(x) paste(head(x, -5), collapse = " "))
# Volver strings en objetos tipo Corpus para funciones con la librería
cuentos <- Corpus(VectorSource(cuentos))
# Quitar puntuacion, números, espacios dobles, y pasar todo a minuscula
cuentos <- tm_map(cuentos,content_transformer(removePunctuation))
cuentos <- tm_map(cuentos,content_transformer(removeNumbers))
cuentos <- tm_map(cuentos,content_transformer(stripWhitespace))
cuentos <- tm_map(cuentos, content_transformer(tolower))
# Lista stopwords español de dos fuentes diferentes y combinamos
lista_palabras1 <- stopwords(language = "es", source = "snowball")
lista_palabras2 <- stopwords(language = "es", source = "nltk")
lista_palabras <- union(lista_palabras1, lista_palabras2)
# TODO: podría quitarse nombres o cosas recurrentes
# (Nombres buscar palabras empiezan mayúsculas y revisar) (revisar todos terminan con Hérnan Casciari)
# También lenguaje argento
# Agregamos palabras específica
lista_palabras <- c(lista_palabras,"mientras")
cuentos<-tm_map(cuentos, removeWords, lista_palabras)
# proximo miércoles hay actividad
# TODO cambiar esta parte
# Expresiones regulares para cambiar "miniseriedeTV" por "miniserie tv"
toTV <- content_transformer(function(x, pattern) {return (gsub(pattern, 'tv', x))})
# CORRECCIÓN DEL ERROR: Cambiar 'sinopsis' por 'cuentos'
cuentos <- tm_map(cuentos, toTV, "miniseriedetv")
cuentos <- tm_map(cuentos, toTV, "miniserie tv")
# Verificar contenido (ajustar índice si hay menos de 101 documentos)
if(length(cuentos) >= 101) {
cuentos[[101]]$content
} else {
cat("Menos de 101 documentos. Mostrando el último:\n")
print(cuentos[[length(cuentos)]]$content)
}
# Lematizar (modelo de udp ya descargado sección previa)
# Convertir corpus a data.frame temporal
texto_df <- data.frame(texto = sapply(cuentos, as.character), stringsAsFactors = FALSE)
cat("Procesando", nrow(texto_df), "documentos con udpipe...\n")
# Procesar con udpipe
anotaciones <- udpipe_annotate(ud_model, x = texto_df$texto)
anotaciones <- as.data.frame(anotaciones)
# Usar solo las lemas (en minúsculas)
cuentos_lematizados <- anotaciones %>%
group_by(doc_id) %>%
summarise(texto = paste(lemma, collapse = " "), .groups = "drop") %>%
pull(texto)
# Volver a objeto Corpus
cuentos <- Corpus(VectorSource(cuentos_lematizados))
# Convertimos el corpus lematizado a vector de texto
textos_lem <- sapply(cuentos, as.character)
cat("Generando n-gramas...\n")
# Generamos n-gramas de 1, 2 y 3 palabras
tokens_unigramas <- tokenize_words(textos_lem)
tokens_bigramas  <- tokenize_ngrams(textos_lem, n = 2)
tokens_trigramas <- tokenize_ngrams(textos_lem, n = 3)
# Combinamos todos los tokens en un solo texto por documento
tokens_combinados <- mapply(function(u, b, t) {
paste(c(u, b, t), collapse = " ")
}, tokens_unigramas, tokens_bigramas, tokens_trigramas)
# Volvemos al formato Corpus para continuar el pipeline
cuentos <- Corpus(VectorSource(tokens_combinados))
cat("Creando matriz documento-término...\n")
# Tokenizar (crear matriz documentos)
dtm_cuentos <- DocumentTermMatrix(cuentos)
cat("Dimensiones DTM original:", dim(dtm_cuentos), "\n")
# Quitar sparsity; quitar palabras que no están en 90% docs
dtm_cuentos <- removeSparseTerms(dtm_cuentos, sparse = 0.90)
cat("Dimensiones DTM después de sparse removal:", dim(dtm_cuentos), "\n")
# Crear directorio si no existe
dir.create("stores", showWarnings = FALSE)
# Exportar a stores
saveRDS(dtm_cuentos, file = "stores/dtm_cuentos.rds")
cat("Proceso completado. DTM guardada en stores/dtm_cuentos.rds\n")
cat("Documentos finales:", nrow(dtm_cuentos), "\n")
cat("Vocabulario final:", ncol(dtm_cuentos), "términos\n")
# =============================================================
# MOSTRAR MÉTRICAS EN PANTALLA
# Agregar al final de tu pipeline (después de crear dtm_cuentos)
# =============================================================
cat("\n===============================================\n")
cat("           MÉTRICAS DEL PIPELINE\n")
cat("===============================================\n")
# 1. Obtener DTM original para comparar
dtm_original <- DocumentTermMatrix(cuentos)
# 2. Extraer números clave
docs_finales <- nrow(dtm_cuentos)
terminos_finales <- ncol(dtm_cuentos)
terminos_originales <- ncol(dtm_original)
reduccion_absoluta <- terminos_originales - terminos_finales
reduccion_pct <- round((reduccion_absoluta / terminos_originales) * 100, 1)
# 3. Top términos
frecuencias <- colSums(as.matrix(dtm_cuentos))
top_terminos <- sort(frecuencias, decreasing = TRUE)
# 4. Sparsity real
sparsity_real <- round((1 - sum(dtm_cuentos > 0) / (docs_finales * terminos_finales)) * 100, 1)
# 5. MOSTRAR TABLA PRINCIPAL
cat("DIMENSIONES Y REDUCCIÓN:\n")
cat("========================\n")
cat("Documentos finales:              ", docs_finales, "\n")
cat("Términos originales:             ", terminos_originales, "\n")
cat("Términos finales:                ", terminos_finales, "\n")
cat("Términos eliminados:             ", reduccion_absoluta, "\n")
cat("Reducción vocabulario:           ", reduccion_pct, "%\n")
cat("Sparsity aplicada:               90%\n")
cat("Sparsity real resultante:        ", sparsity_real, "%\n")
# 6. MOSTRAR TOP TÉRMINOS
cat("\nTOP 10 TÉRMINOS MÁS FRECUENTES:\n")
cat("===============================\n")
for(i in 1:min(10, length(top_terminos))) {
cat(sprintf("%2d. %-15s (%3d docs)\n", i, names(top_terminos)[i], top_terminos[i]))
}
# 7. TEXTO LISTO PARA REPORTE
cat("\n===============================================\n")
cat("           TEXTO PARA TU REPORTE\n")
cat("===============================================\n")
texto_reporte <- sprintf(
"La matriz documento-término final presenta dimensiones de %d documentos × %d términos tras aplicar un umbral de sparsity del 90%%. El vocabulario se redujo de %d a %d términos únicos, representando una disminución del %s%% que optimiza la representación semántica. Los términos más frecuentes post-procesamiento son: %s.",
docs_finales,
terminos_finales,
terminos_originales,
terminos_finales,
reduccion_pct,
paste(names(top_terminos)[1:5], collapse = ", ")
)
cat(texto_reporte, "\n")
cat("\n===============================================\n")
# =============================================================
# Recomendadores: este archivo es la segunda parte de proyecto.
#            Ahora se desarrollan 2 metodologías de recomendación
#            de cuentos del escritor Hernán Casciari. Primero se
#            implementa uno basado en TF-IDF y similitud del coseno,
#            luego uno basado en LDA.
#
# 1) Buenas prácticas, librerías y descarga de datos
# 2) Recomendador TF-IDF y similitud del coseno
# 3) Recomendador LDA
# 4)
#
# =============================================================
# =============================================================
# 1) Buenas prácticas, librerías y descarga de datos
# =============================================================
# Buenas prácticas
rm(list = ls())
# NOTA: CAMBIAR DIRECTORIO
# setwd("C:/Users/Asuar/OneDrive/Escritorio/Libros Clases/Economía/Ciencia Datos y Econometria/Taller3-blog_casciari")
# Librerías
require(pacman)
p_load(tidyverse, stringr, dplyr, tm, proxy) #TODO para procesar data
# Datos y primera visualización
db <- readRDS("stores/dtm_cuentos.rds")
# =============================================================
# 2) Recomendador TF-IDF y similitud del coseno
# =============================================================
# Crear matriz TF-IDF a partir del corpus de cuentos
tfidf <- weightTfIdf(db)
# Matriz TF-IDF
tfidf_matrix <- as.matrix(tfidf)
# TODO Se podría normalizar los vectores antes de similitud para controlar efecto por longitud
tfidf_matrix <- tfidf_matrix / sqrt(rowSums(tfidf_matrix^2))
# Calcular similitud de coseno entre documentos
cosine_sim <- 1 - proxy::dist(tfidf_matrix, tfidf_matrix, method = "cosine")
# TODO Revisar un ejemplo rápido (opcional)
cosine_sim[1, 1:10]
# Crear función recomendadora
# TODO revisar que orden en dtm sea el mismo que en db
recomendador_tfidf <- function(title, distancias = cosine_sim, df = db) {
# Obtener el índice del cuento
idx <- df$index[df$titulo == title]
if (length(idx) == 0) {
stop("El título ingresado no existe en la base de datos.")
}
# Crear un data.frame con las distancias y los títulos
recomendaciones <- data.frame(
dist = distancias[idx, ],
titulo = df$titulo
)
# Ordenar de mayor a menor similitud
recomendaciones <- recomendaciones %>% arrange(-dist)
# Retornar los 10 cuentos más similares (excluyendo el mismo)
return(recomendaciones$titulo[2:11])
}
# Ejemplo de uso
recomendador_tfidf("Messi es un perro")
# =============================================================
# Recomendadores: este archivo es la segunda parte de proyecto.
#            Ahora se desarrollan 2 metodologías de recomendación
#            de cuentos del escritor Hernán Casciari. Primero se
#            implementa uno basado en TF-IDF y similitud del coseno,
#            luego uno basado en LDA.
#
# 1) Buenas prácticas, librerías y descarga de datos
# 2) Recomendador TF-IDF y similitud del coseno
# 3) Recomendador LDA
# 4)
#
# =============================================================
# =============================================================
# 1) Buenas prácticas, librerías y descarga de datos
# =============================================================
# Buenas prácticas
rm(list = ls())
# NOTA: CAMBIAR DIRECTORIO
# setwd("C:/Users/Asuar/OneDrive/Escritorio/Libros Clases/Economía/Ciencia Datos y Econometria/Taller3-blog_casciari")
# Librerías
require(pacman)
p_load(tidyverse, stringr, dplyr, tm, proxy) #TODO para procesar data
# Datos y primera visualización
db <- readRDS("stores/dtm_cuentos.rds")
# =============================================================
# 2) Recomendador TF-IDF y similitud del coseno
# =============================================================
# Crear matriz TF-IDF a partir del corpus de cuentos
tfidf <- weightTfIdf(db)
# Matriz TF-IDF
tfidf_matrix <- as.matrix(tfidf)
# TODO Se podría normalizar los vectores antes de similitud para controlar efecto por longitud
tfidf_matrix <- tfidf_matrix / sqrt(rowSums(tfidf_matrix^2))
# Calcular similitud de coseno entre documentos
cosine_sim <- 1 - proxy::dist(tfidf_matrix, tfidf_matrix, method = "cosine")
# TODO Revisar un ejemplo rápido (opcional)
cosine_sim[1, 1:10]
# Crear función recomendadora
# TODO revisar que orden en dtm sea el mismo que en db
recomendador_tfidf <- function(title, distancias = cosine_sim, df = db) {
# Obtener el índice del cuento
idx <- df$index[df$titulo == title]
if (length(idx) == 0) {
stop("El título ingresado no existe en la base de datos.")
}
# Crear un data.frame con las distancias y los títulos
recomendaciones <- data.frame(
dist = distancias[idx, ],
titulo = df$titulo
)
# Ordenar de mayor a menor similitud
recomendaciones <- recomendaciones %>% arrange(-dist)
# Retornar los 10 cuentos más similares (excluyendo el mismo)
return(recomendaciones$titulo[2:11])
}
# Ejemplo de uso
recomendador_tfidf("Messi es un perro")
# =============================================================
# Recomendadores: este archivo es la segunda parte de proyecto.
#            Ahora se desarrollan 2 metodologías de recomendación
#            de cuentos del escritor Hernán Casciari. Primero se
#            implementa uno basado en TF-IDF y similitud del coseno,
#            luego uno basado en LDA.
#
# 1) Buenas prácticas, librerías y descarga de datos
# 2) Recomendador TF-IDF y similitud del coseno
# 3) Recomendador LDA
# 4)
#
# =============================================================
# =============================================================
# 1) Buenas prácticas, librerías y descarga de datos
# =============================================================
# Buenas prácticas
rm(list = ls())
# NOTA: CAMBIAR DIRECTORIO
# setwd("C:/Users/Asuar/OneDrive/Escritorio/Libros Clases/Economía/Ciencia Datos y Econometria/Taller3-blog_casciari")
# Librerías
require(pacman)
p_load(tidyverse, stringr, dplyr, tm, proxy) #TODO para procesar data
# Datos y primera visualización
db <- readRDS("stores/dtm_cuentos.rds")
# =============================================================
# 2) Recomendador TF-IDF y similitud del coseno
# =============================================================
# Crear matriz TF-IDF a partir del corpus de cuentos
tfidf <- weightTfIdf(db)
# Matriz TF-IDF
tfidf_matrix <- as.matrix(tfidf)
# TODO Se podría normalizar los vectores antes de similitud para controlar efecto por longitud
tfidf_matrix <- tfidf_matrix / sqrt(rowSums(tfidf_matrix^2))
# Calcular similitud de coseno entre documentos
cosine_sim <- 1 - proxy::dist(tfidf_matrix, tfidf_matrix, method = "cosine")
# TODO Revisar un ejemplo rápido (opcional)
cosine_sim[1, 1:10]
# Crear función recomendadora
# TODO revisar que orden en dtm sea el mismo que en db
recomendador_tfidf <- function(title, distancias = cosine_sim, df = db) {
# Obtener el índice del cuento
idx <- df$index[df$titulo == title]
#if (length(idx) == 0) {
#  stop("El título ingresado no existe en la base de datos.")
#}
# Crear un data.frame con las distancias y los títulos
recomendaciones <- data.frame(
dist = distancias[idx, ],
titulo = df$titulo
)
# Ordenar de mayor a menor similitud
recomendaciones <- recomendaciones %>% arrange(-dist)
# Retornar los 10 cuentos más similares (excluyendo el mismo)
return(recomendaciones$titulo[2:11])
}
# Ejemplo de uso
recomendador_tfidf("Messi es un perro")
# =============================================================
# Recomendadores: este archivo es la segunda parte de proyecto.
#            Ahora se desarrollan 2 metodologías de recomendación
#            de cuentos del escritor Hernán Casciari. Primero se
#            implementa uno basado en TF-IDF y similitud del coseno,
#            luego uno basado en LDA.
#
# 1) Buenas prácticas, librerías y descarga de datos
# 2) Recomendador TF-IDF y similitud del coseno
# 3) Recomendador LDA
# 4)
#
# =============================================================
exit()
clear
