# agregar columna mes con el nombre de la carpeta (sin prefijo CSV_)
mes_name <- sub('CSV_', '', basename(d))
df$mes <- mes_name
tablas_mes[[length(tablas_mes) + 1]] <- df
origen <- c(origen, mes_name)
}
}
# Asignar a .GlobalEnv para que queden disponibles tras source() en RStudio
assign('datos', datos, envir = .GlobalEnv)
assign('missing_report', missing_report, envir = .GlobalEnv)
if(debug) cat('\nObjetos `datos` y `missing_report` guardados en el Global Environment.\n')
if(length(tablas_mes) > 0){
# Usar rbind.fill-like behavior: convertir a data.frame con columnas unidas por nombre
# Implementación simple: usar plyr::rbind.fill si disponible, sino base with missing cols
if(requireNamespace('plyr', quietly = TRUE)){
combined <- plyr::rbind.fill(tablas_mes)
} else {
# Alinear columnas
all_cols <- unique(unlist(lapply(tablas_mes, names)))
tablas_adj <- lapply(tablas_mes, function(x){
missing <- setdiff(all_cols, names(x))
for(mc in missing) x[[mc]] <- NA
# ordenar columnas
x[all_cols]
})
combined <- do.call(rbind, tablas_adj)
rownames(combined) <- NULL
}
datos[[tools::file_path_sans_ext(f)]] <- combined
if(debug) cat(sprintf('Combinado %s: %d filas, %d columnas\n', tools::file_path_sans_ext(f), nrow(combined), ncol(combined)))
}
}
# ===================================================================
# (continuación) 2) Reporte de porcentaje de ausencias por variable (NA o cadena vacía)
# ===================================================================
missing_report <- list()
is_missing_vec <- function(x){
if(is.factor(x)) x <- as.character(x)
if(is.character(x)){
return(is.na(x) | trimws(x) == '')
}
is.na(x)
}
for(name in names(datos)){
df <- datos[[name]]
n <- nrow(df)
if(n == 0){
missing_report[[name]] <- data.frame(variable = character(0), pct_missing = numeric(0), stringsAsFactors = FALSE)
next
}
vars <- names(df)
pct <- sapply(vars, function(v){
miss <- sum(is_missing_vec(df[[v]]))
round(100 * miss / n, 2)
})
missing_report[[name]] <- data.frame(variable = vars, pct_missing = as.numeric(pct), stringsAsFactors = FALSE)
}
# Al final, tenemos 'datos' (lista de tablas combinadas por tipo de archivo)
# y 'missing_report' con porcentajes de ausencia por variable para cada tabla.
if(debug){
cat('\nResumen de ausencias por tabla:\n')
for(nm in names(missing_report)){
cat(sprintf('\n== %s ==\n', nm))
dfm <- missing_report[[nm]]
# mostrar las variables con mayor ausencia (top 10)
dfm <- dfm[order(-dfm$pct_missing),]
print(utils::head(dfm, 20))
}
}
ls()
setwd()
ls()
install.packages("pacman")
require(pacman)
p_load(textir)
data("we8there", package = "textir")
dta<- we9thereCounts
require(pacman)
p_load(textir)
data("we8there", package = "textir")
dta<- we9thereCounts
dta<- we8thereCounts
class(dta)
dim(dta)
dta(1,)
pca <- prcomp(dta,scale=TRUE)
library(dplyr)
get_top_words <- function(loadings, comp, n = 10) {
tibble(
palabra = names(sort(abs(loadings[, comp]), decreasing = TRUE))[1:n],
peso = sort(loadings[, comp], decreasing = TRUE)[1:n]
)
}
top_pc1 <- get_top_words(pca$rotation, 1)
top_pc2 <- get_top_words(pca$rotation, 2)
top_pc1
View(top_pc1)
p_load(wordcloud)
# Nube para el primer componente (palabras con pesos positivos)
loadings_pc1 <- sort(pca$rotation[,1], decreasing = TRUE)
wordcloud(names(loadings_pc1), loadings_pc1, max.words = 40, colors = "steelblue")
wordcloud(names(loadings_pc1), loadings_pc1, max.words = 40, colors = "steelblue")
biplot(pca, scale = 0, cex = 0.6)
biplot(pca, scale = 0, cex = 0.6)
plot(pca$x[, 1], pca$x[, 2],
xlab = "PC1: tono positivo vs negativo",
ylab = "PC2: servicio vs comida",
main = "Documentos (reseñas) en el espacio PCA",
pch = 19, col = rgb(0, 0, 0.7, 0.4))
library(devtools)
install.packages("devtools")
install.packages("devtools")
install.packages("devtools")
install.packages("devtools")
library(devtools)
install_github("topepo/caret/pkg/caret")
View(top_pc1)
setwd("C:/Users/Asuar/OneDrive/Escritorio/Libros Clases/Economía/Ciencia Datos y Econometria/Taller3-blog_casciari")
# Librerías
require(pacman)
p_load(tidyverse, stringr, dplyr, stringi, tm, stopwords, tokenizers) #TODO para procesar data
p_load(rvest, udpipe) # para web scrapping
# Datos y primera visualización
db <- read.csv("stores/blog_casciari.csv")
ud_model_file <- udpipe_download_model(language = "spanish")
ud_model <- udpipe_load_model(ud_model_file$file_model)
# Eliminamos tildes y caracteres especiales del español
cuentos <- stri_trans_general(str = db$cuento, id = "Latin-ASCII")
# Quitar "/n" y reemplazar por espacio
cuentos <- str_replace_all(cuentos, "\n"," ")
# Quitar últimas 5 palabras, pues son nombre autor y fecha
cuentos <- sapply(str_split(cuentos, "\\s+"), function(x) paste(head(x, -5), collapse = " "))
# Volver strings en objetos tipo Corpus para funciones con la librería
cuentos <- Corpus(VectorSource(cuentos))
# Quitar puntuacion, números, espacios dobles, y pasar todo a minuscula
cuentos <- tm_map(cuentos,content_transformer(removePunctuation))
cuentos <- tm_map(cuentos,content_transformer(removeNumbers))
cuentos <- tm_map(cuentos,content_transformer(stripWhitespace))
cuentos <- tm_map(cuentos, content_transformer(tolower))
# Lista stopwords español de dos fuentes diferentes y combinamos
lista_palabras1 <- stopwords(language = "es", source = "snowball")
lista_palabras2 <- stopwords(language = "es", source = "nltk")
lista_palabras <- union(lista_palabras1, lista_palabras2)
texto_df <- data.frame(texto = sapply(cuentos, as.character), stringsAsFactors = FALSE)
# Procesar con udpipe
anotaciones <- udpipe_annotate(ud_model, x = texto_df$texto)
anotaciones <- as.data.frame(anotaciones)
# Usar solo las lemas (en minúsculas)
cuentos_lematizados <- anotaciones %>%
group_by(doc_id) %>%
summarise(texto = paste(lemma, collapse = " ")) %>%
pull(texto)
# Volver a objeto Corpus
cuentos <- Corpus(VectorSource(cuentos_lematizados))
# Convertimos el corpus lematizado a vector de texto
textos_lem <- sapply(cuentos, as.character)
# Generamos n-gramas de 1, 2 y 3 palabras
tokens_unigramas <- tokenize_words(textos_lem)
tokens_bigramas  <- tokenize_ngrams(textos_lem, n = 2)
tokens_trigramas <- tokenize_ngrams(textos_lem, n = 3)
# Combinamos todos los tokens en un solo texto por documento
tokens_combinados <- mapply(function(u, b, t) {
paste(c(u, b, t), collapse = " ")
}, tokens_unigramas, tokens_bigramas, tokens_trigramas)
# Volvemos al formato Corpus para continuar el pipeline
cuentos <- Corpus(VectorSource(tokens_combinados))
# Tokenizar (crear matriz documentos)
dtm_cuentos<-DocumentTermMatrix(cuentos)
# Quitar sparsity; quitar palabras que no están en 90% docs
dtm_cuentos <- removeSparseTerms(dtm_cuentos, sparse = 0.90)
# Exportar a stores
saveRDS(dtm_cuentos, file = "stores/dtm_cuentos.rds")
db <- readRDS("stores/dtm_cuentos.rds")
# Crear matriz TF-IDF a partir del corpus de cuentos
tfidf <- weightTfIdf(db)
# Matriz TF-IDF
tfidf_matrix <- as.matrix(tfidf)
View(tfidf_matrix)
# Calcular similitud de coseno entre documentos
cosine_sim <- 1 - proxy::dist(tfidf_matrix, tfidf_matrix, method = "cosine")
# TODO Revisar un ejemplo rápido (opcional)
cosine_sim[1, 1:10]
# Crear función recomendadora
# TODO revisar que orden en dtm sea el mismo que en db
recomendador_tfidf <- function(title, distancias = cosine_sim, df = db) {
# Obtener el índice del cuento
idx <- df$index[df$titulo == title]
if (length(idx) == 0) {
stop("El título ingresado no existe en la base de datos.")
}
# Crear un data.frame con las distancias y los títulos
recomendaciones <- data.frame(
dist = distancias[idx, ],
titulo = df$titulo
)
# Ordenar de mayor a menor similitud
recomendaciones <- recomendaciones %>% arrange(-dist)
# Retornar los 10 cuentos más similares (excluyendo el mismo)
return(recomendaciones$titulo[2:11])
}
# Ejemplo de uso
recomendador_tfidf("Messi es un perro")
db_prueba <- read.csv("stores/blog_casciari.csv")
View(db_prueba)
db_prueba$titulo["Messi es un perro"]
# Ejemplo de uso
recomendador_tfidf("Los dos rulfos")
# Ejemplo de uso
recomendador_tfidf(Los dos rulfos)
# Ejemplo de uso
recomendador_tfidf("Los dos rulfos")
# Ejemplo de uso
recomendador_tfidf("El milagro de los pueblos")
View(db)
# Datos y primera visualización
cuentos <- readRDS("stores/dtm_cuentos.rds") # cuentos procesados
db <- read.csv("stores/blog_casciari.csv") # db original para cuentos
recomendador_tfidf <- function(title, distancias = cosine_sim, df = db) {
# Obtener el índice del cuento
idx <- df$index[df$titulo == title]
if (length(idx) == 0) {
stop("El título ingresado no existe en la base de datos.")
}
# Crear un data.frame con las distancias y los títulos
recomendaciones <- data.frame(
dist = distancias[idx, ],
titulo = df$titulo
)
# Ordenar de mayor a menor similitud
recomendaciones <- recomendaciones %>% arrange(-dist)
# Retornar los 10 cuentos más similares (excluyendo el mismo)
return(recomendaciones$titulo[2:11])
}
# Ejemplo de uso
recomendador_tfidf("Messi es un perro")
recomendador_tfidf("El rincón blanco")
db$index <- 1:nrow(db)
# =============================================================
# Recomendadores: este archivo es la segunda parte de proyecto.
#            Ahora se desarrollan 2 metodologías de recomendación
#            de cuentos del escritor Hernán Casciari. Primero se
#            implementa uno basado en TF-IDF y similitud del coseno,
#            luego uno basado en LDA.
#
# 1) Buenas prácticas, librerías y descarga de datos
# 2) Recomendador TF-IDF y similitud del coseno
# 3) Recomendador LDA
# 4)
#
# =============================================================
# =============================================================
# 1) Buenas prácticas, librerías y descarga de datos
# =============================================================
# Buenas prácticas
rm(list = ls())
# NOTA: CAMBIAR DIRECTORIO
# setwd("C:/Users/Asuar/OneDrive/Escritorio/Libros Clases/Economía/Ciencia Datos y Econometria/Taller3-blog_casciari")
# Librerías
require(pacman)
p_load(tidyverse, stringr, dplyr, tm, proxy) #TODO para procesar data
# Datos y primera visualización
cuentos <- readRDS("stores/dtm_cuentos.rds") # cuentos procesados
db <- read.csv("stores/blog_casciari.csv") # db original para cuentos
# =============================================================
# 2) Recomendador TF-IDF y similitud del coseno
# =============================================================
# Crear matriz TF-IDF a partir del corpus de cuentos
tfidf <- weightTfIdf(cuentos)
# Matriz TF-IDF
tfidf_matrix <- as.matrix(tfidf)
# TODO Se podría normalizar los vectores antes de similitud para controlar efecto por longitud
tfidf_matrix <- tfidf_matrix / sqrt(rowSums(tfidf_matrix^2))
# Calcular similitud de coseno entre documentos
cosine_sim <- 1 - proxy::dist(tfidf_matrix, tfidf_matrix, method = "cosine")
# TODO Revisar un ejemplo rápido (opcional)
cosine_sim[1, 1:10]
# Asegurar que 'db' tenga una columna de índice (para vincular títulos y filas del DTM)
db$index <- 1:nrow(db)
# Crear función recomendadora
# TODO revisar que orden en dtm sea el mismo que en db
recomendador_tfidf <- function(title, distancias = cosine_sim, df = db) {
# Obtener el índice del cuento
idx <- df$index[df$titulo == title]
if (length(idx) == 0) {
stop("El título ingresado no existe en la base de datos.")
}
# Crear un data.frame con las distancias y los títulos
recomendaciones <- data.frame(
dist = as.nummeric(distancias[idx, ]),
titulo = df$titulo
)
# Ordenar de mayor a menor similitud
recomendaciones <- recomendaciones %>% arrange(desc(-dist))
# Retornar los 10 cuentos más similares (excluyendo el mismo)
return(recomendaciones$titulo[2:11])
}
# Ejemplo de uso
recomendador_tfidf("Messi es un perro")
recomendador_tfidf <- function(title, distancias = cosine_sim, df = db) {
# Obtener el índice del cuento
idx <- df$index[df$titulo == title]
if (length(idx) == 0) {
stop("El título ingresado no existe en la base de datos.")
}
# Crear un data.frame con las distancias y los títulos
recomendaciones <- data.frame(
dist = as.numeric(distancias[idx, ]),
titulo = df$titulo
)
# Ordenar de mayor a menor similitud
recomendaciones <- recomendaciones %>% arrange(desc(-dist))
# Retornar los 10 cuentos más similares (excluyendo el mismo)
return(recomendaciones$titulo[2:11])
}
# Ejemplo de uso
recomendador_tfidf("Messi es un perro")
c1 <- db %>% filter(titulo == "Messi es un perro") %>% pull(cuento)
c2 <- db %>% filter(titulo == "Caminos del papel moderno") %>% pull(cuento)
c1
c2
c2 <- db %>% filter(titulo == "Matar la crisis a volantazos") %>% pull(cuento)
c2
# Ciencia de Datos Actividad recomendación autores
# Cargamos los paquetes
require("pacman")
p_load("tidyverse","tidytext")
# Cargamos los datos
ensayos <- read.csv("https://raw.githubusercontent.com/ignaciomsarmiento/datasets/main/ensayos.csv")
# Visualizamos las primeras filas
head(ensayos)
#
glimpse(ensayos)
#
table(ensayos$titulo)
#
p_load("stringi","tm")
# Eliminamos tíldes y caracteres especiales del español
docs <- stri_trans_general(str = ensayos$texto, id = "Latin-ASCII")
docs[1]
#
texto <- Corpus(VectorSource(docs))
texto
# Minuscula y puntuación
texto <- tm_map(texto, content_transformer(tolower))
texto <- tm_map(texto, content_transformer(removePunctuation))
texto[[1]]$content
# Cargamos stopwords
p_load(stopwords)
# Descargamos la lista de las stopwords en español de dos fuentes diferentes y las combinamos
lista_palabras1 <- stopwords(language = "es", source = "snowball")
lista_palabras2 <- stopwords(language = "es", source = "nltk")
lista_palabras <- union(lista_palabras1, lista_palabras2)
texto <- tm_map(texto, removeWords, lista_palabras)
texto[[1]]$content
lista_palabras <- c(lista_palabras,
"freud", "sigmund", "voltaire","avram", "chomsky",
"noam", "psicoanalisis", "filosofo", "linguistica",
"ensayo", "articulo", "texto", "autor", "pagina",
"capitulo", "seccion", "introduccion", "conclusion")
texto<-tm_map(texto,removeWords,lista_palabras)
texto[[1]]$content
texto <- tm_map(texto,content_transformer(stripWhitespace))
texto[[1]]$content
p_load("textstem")
texto <- tm_map(texto, lemmatize_strings)
texto[[1]]$content
dtm_texto <- DocumentTermMatrix(texto)
View(dtm_texto)
p_load("maptpx")
# COMPLETA EL CÓDIGO
# Nota: maptpx requiere una matriz, no un DocumentTermMatrix
dtm_matrix <- as.matrix(dtm_texto)
# Establece la semilla para reproducibilidad
set.seed(123)
# Ajusta el modelo usando topics() de maptpx
ensayos_lda <- topics(dtm_matrix, K = 3)
# Examina el objeto
summary(ensayos_lda)
# Ajusta el modelo usando topics() de maptpx
grid <- c(2,3,4,5)
sapply(grid, function(k){
ensayos_lda <- topics(dtm_matrix, K = k)
cat("Número de temas:", k, "\n")
summary(ensayos_lda)
cat("----------------")
})
# Extraemos theta y convertimos a formato tidy
theta_matrix <- ensayos_lda$theta
# Convertimos a formato largo (tidy)
ensayos_topics <- as.data.frame(theta_matrix) %>%
mutate(term = rownames(theta_matrix)) %>%
pivot_longer(cols = -term,
names_to = "topic",
values_to = "beta") %>%
mutate(topic = as.numeric(gsub("V", "", topic)))
# Examina la estructura
head(ensayos_topics)
# COMPLETA EL CÓDIGO
ensayos_top_terms <- ensayos_topics %>%
group_by(5) %>%
slice_max(5, n = 5) %>%
ungroup() %>%
arrange(topic, -beta)
??slice_max
# COMPLETA EL CÓDIGO
ensayos_top_terms <- ensayos_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
# Visualización
ensayos_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered() +
labs(title = "Top 10 términos por tópico",
x = "Beta (probabilidad palabra-tópico)",
y = "Término")
# Extraemos omega y convertimos a formato tidy
omega_matrix <- ensayos_lda$omega
# Convertimos a formato largo (tidy)
ensayos_documents <- as.data.frame(omega_matrix) %>%
mutate(document = row_number()) %>%
pivot_longer(cols = -document,
names_to = "topic",
values_to = "gamma") %>%
mutate(topic = as.numeric(gsub("V", "", topic)))
# Examina algunos documentos
head(ensayos_documents, 15)
View(dtm_texto)
View(ensayos)
# Examina algunos documentos
head(ensayos_documents, -15)
# COMPLETA EL CÓDIGO
ensayos_documents %>%
ggplot(aes(x = factor(topic), y = ensayos_documents, fill = factor(topic))) +
geom_boxplot() +
labs(title = "Distribución de probabilidades γ por tópico",
x = "Tópico",
y = "Probabilidad documento-tópico",
fill = "Tópico")
# COMPLETA EL CÓDIGO
ensayos_documents %>%
ggplot(aes(x = factor(topic), y = ensayos_documents$gamma, fill = factor(topic))) +
geom_boxplot() +
labs(title = "Distribución de probabilidades γ por tópico",
x = "Tópico",
y = "Probabilidad documento-tópico",
fill = "Tópico")
# COMPLETA EL CÓDIGO
ensayos_documents %>%
ggplot(aes(x = factor(topic), y = gamma, fill = factor(topic))) +
geom_boxplot() +
labs(title = "Distribución de probabilidades γ por tópico",
x = "Tópico",
y = "Probabilidad documento-tópico",
fill = "Tópico")
# Reshape a matriz ancha
gamma_wide <- ensayos_documents %>%
pivot_wider(names_from = topic,
values_from = gamma,
names_prefix = "topic_")
# Convierte a matriz (excluyendo la columna document)
gamma_matrix <- as.matrix(gamma_wide[, -1])  # ✓
rownames(gamma_matrix) <- gamma_wide$document # ✓
# Reshape a matriz ancha
gamma_wide <- ensayos_documents %>%
pivot_wider(names_from = topic,
values_from = gamma,
names_prefix = "topic_")
# Convierte a matriz (excluyendo la columna document)
gamma_matrix <- as.matrix(gamma_wide[, -1])  # ✓
rownames(gamma_matrix) <- gamma_wide$document # ✓
# Función de similitud de coseno
cosine_similarity <- function(matrix) {
# Normaliza las filas (norma euclidiana)
norm_matrix <- matrix / sqrt(rowSums(matrix^2))
# Calcula el producto matricial (que da similitud de coseno)
similarity <- norm_matrix %*% t(norm_matrix)
return(similarity)
}
# COMPLETA: Aplica la función
similarity_matrix <- cosine_similarity(gamma_matrix)
# Examina las primeras 5x5 entradas
similarity_matrix[1:5, 1:5]
# COMPLETA EL CÓDIGO
doc1_similarities <- similarity_matrix[1, ]
top5_similar <- sort(doc1_similarities, decreasing = ___)[___]
# COMPLETA EL CÓDIGO
doc1_similarities <- similarity_matrix[1, ]
top5_similar <- sort(doc1_similarities, decreasing = TRUE)[1:5]
print("Top 5 documentos más similares al documento 1:")
print(top5_similar)
# COMPLETA EL CÓDIGO
doc1_similarities <- similarity_matrix[1, ]
top5_similar <- sort(doc1_similarities, decreasing = TRUE)[2:6]
print("Top 5 documentos más similares al documento 1:")
print(top5_similar)
# Convertimos a formato largo (tidy)
ensayos_documents <- as.data.frame(omega_matrix) %>%
mutate(document = row_number()) %>%
pivot_longer(cols = -document,
names_to = "topic",
values_to = "gamma") %>%
mutate(topic = as.numeric(gsub("V", "", topic)))
# Examina algunos documentos
head(ensayos_documents, 100)
print(ensayos_documents)
print(n = 100, ensayos_documents)
print(n = 500, ensayos_documents)
