miss <- sum(is_missing_vec(df[[v]]))
round(100 * miss / n, 2)
})
missing_report[[name]] <- data.frame(variable = vars, pct_missing = as.numeric(pct), stringsAsFactors = FALSE)
}
View(datos)
View(missing_report)
# ===================================================================
# Archivo 1: cargar los datos de la GEIH en una estructura de datos
#            y aplicar transformaciones para su posterior análisis
#
# 1) Buenas práctias y librerías
# 2) Cargar los datos de la GEIH
# 3) Renombrar variables
# 4) Determinar varibles por atrición (>40% atrición)
# 5) Creación de variables propias
# ===================================================================
# ===================================================================
# 1) Buenas prácticas y librerías
# ===================================================================
# Limpiar el entorno de R
rm(list = ls())
# Cargar librerías
require("pacman")
pload() #TODO
# ===================================================================
# 2) Cargar los datos de la GEIH
# ===================================================================
# Ruta relativa a las carpetas con datos (asumiendo este script en scripts/)
base_dir <- file.path('..','GEIH-2025')
# Buscar carpetas que sigan el patrón CSV_mes (enero..julio)
mes_dirs <- list.dirs(path = base_dir, full.names = TRUE, recursive = FALSE)
mes_dirs <- mes_dirs[grepl('CSV_', basename(mes_dirs))]
# Función utilitaria: leer un CSV con encoding y tipos flexibles
safe_read_csv <- function(path){
# usar read.csv base para compatibilidad; stringsAsFactors = FALSE
tryCatch(
read.csv(path, fileEncoding = 'UTF-8', stringsAsFactors = FALSE, check.names = TRUE),
error = function(e) read.csv(path, fileEncoding = 'latin1', stringsAsFactors = FALSE, check.names = TRUE)
)
}
# Reunir lista de nombres de archivos (asumiendo mismos nombres en cada mes)
all_files <- unique(unlist(lapply(mes_dirs, function(d) basename(list.files(d, pattern = '\\.[Cc][Ss][Vv]$', full.names = FALSE)))))
# Lista para guardar tablas combinadas
datos <- list()
for(f in all_files){
# para cada archivo (por ejemplo 'Ocupados.CSV') leer la versión de cada mes y apilar
tablas_mes <- list()
origen <- c()
for(d in mes_dirs){
fp <- file.path(d, f)
if(file.exists(fp)){
df <- safe_read_csv(fp)
# agregar columna mes con el nombre de la carpeta (sin prefijo CSV_)
mes_name <- sub('CSV_', '', basename(d))
df$mes <- mes_name
tablas_mes[[length(tablas_mes) + 1]] <- df
origen <- c(origen, mes_name)
}
}
if(length(tablas_mes) > 0){
# Usar rbind.fill-like behavior: convertir a data.frame con columnas unidas por nombre
# Implementación simple: usar plyr::rbind.fill si disponible, sino base with missing cols
if(requireNamespace('plyr', quietly = TRUE)){
combined <- plyr::rbind.fill(tablas_mes)
} else {
# Alinear columnas
all_cols <- unique(unlist(lapply(tablas_mes, names)))
tablas_adj <- lapply(tablas_mes, function(x){
missing <- setdiff(all_cols, names(x))
for(mc in missing) x[[mc]] <- NA
# ordenar columnas
x[all_cols]
})
combined <- do.call(rbind, tablas_adj)
rownames(combined) <- NULL
}
datos[[tools::file_path_sans_ext(f)]] <- combined
}
}
# ===================================================================
# (continuación) 2) Reporte de porcentaje de ausencias por variable (NA o cadena vacía)
# ===================================================================
missing_report <- list()
is_missing_vec <- function(x){
if(is.factor(x)) x <- as.character(x)
if(is.character(x)){
return(is.na(x) | trimws(x) == '')
}
is.na(x)
}
for(name in names(datos)){
df <- datos[[name]]
n <- nrow(df)
if(n == 0){
missing_report[[name]] <- data.frame(variable = character(0), pct_missing = numeric(0), stringsAsFactors = FALSE)
next
}
vars <- names(df)
pct <- sapply(vars, function(v){
miss <- sum(is_missing_vec(df[[v]]))
round(100 * miss / n, 2)
})
missing_report[[name]] <- data.frame(variable = vars, pct_missing = as.numeric(pct), stringsAsFactors = FALSE)
}
# Al final, tenemos 'datos' (lista de tablas combinadas por tipo de archivo)
# y 'missing_report' con porcentajes de ausencia por variable para cada tabla.
# Ruta relativa a las carpetas con datos (asumiendo este script en scripts/)
base_dir <- file.path('..','GEIH-2025')
# Buscar carpetas que sigan el patrón CSV_mes (enero..julio)
mes_dirs <- c("../GEIH-2025/CSV_enero",
"../GEIH-2025/CSV_febrero",
"../GEIH-2025/CSV_marzo",
"../GEIH-2025/CSV_abril",
"../GEIH-2025/CSV_mayo",
"../GEIH-2025/CSV_junio",
"../GEIH-2025/CSV_julio")
# Función utilitaria: leer un CSV con encoding y tipos flexibles
safe_read_csv <- function(path){
# usar readr::read_csv para lecturas más robustas
tryCatch(
readr::read_csv(path, locale = readr::locale(encoding = 'UTF-8'), show_col_types = FALSE),
error = function(e) readr::read_csv(path, locale = readr::locale(encoding = 'Latin1'), show_col_types = FALSE)
)
# Función utilitaria: leer un CSV con encoding y tipos flexibles
safe_read_csv <- function(path){
# usar readr::read_csv para lecturas más robustas
tryCatch(
readr::read_csv(path, locale = readr::locale(encoding = 'UTF-8'), show_col_types = FALSE),
error = function(e) readr::read_csv(path, locale = readr::locale(encoding = 'Latin1'), show_col_types = FALSE)
)
}
all_files <- unique(unlist(lapply(mes_dirs, function(d) basename(list.files(d, pattern = '\\.[Cc][Ss][Vv]$', full.names = FALSE)))))
# Lista para guardar tablas combinadas
datos <- list()
if(debug) cat(sprintf('Detectados %d archivos distintos en las carpetas de meses:\n', length(all_files)))
if(debug) print(all_files)
for(f in all_files){
# para cada archivo (por ejemplo 'Ocupados.CSV') leer la versión de cada mes y apilar
tablas_mes <- list()
origen <- c()
for(d in mes_dirs){
fp <- file.path(d, f)
if(file.exists(fp)){
if(debug) cat(sprintf('Leyendo: %s\n', fp))
df <- safe_read_csv(fp)
# agregar columna mes con el nombre de la carpeta (sin prefijo CSV_)
mes_name <- sub('CSV_', '', basename(d))
df$mes <- mes_name
tablas_mes[[length(tablas_mes) + 1]] <- df
origen <- c(origen, mes_name)
}
}
if(length(tablas_mes) > 0){
# Usar rbind.fill-like behavior: convertir a data.frame con columnas unidas por nombre
# Implementación simple: usar plyr::rbind.fill si disponible, sino base with missing cols
if(requireNamespace('plyr', quietly = TRUE)){
combined <- plyr::rbind.fill(tablas_mes)
} else {
# Alinear columnas
all_cols <- unique(unlist(lapply(tablas_mes, names)))
tablas_adj <- lapply(tablas_mes, function(x){
missing <- setdiff(all_cols, names(x))
for(mc in missing) x[[mc]] <- NA
# ordenar columnas
x[all_cols]
})
combined <- do.call(rbind, tablas_adj)
rownames(combined) <- NULL
}
datos[[tools::file_path_sans_ext(f)]] <- combined
if(debug) cat(sprintf('Combinado %s: %d filas, %d columnas\n', tools::file_path_sans_ext(f), nrow(combined), ncol(combined)))
}
}
for(f in all_files){
# para cada archivo (por ejemplo 'Ocupados.CSV') leer la versión de cada mes y apilar
tablas_mes <- list()
origen <- c()
for(d in mes_dirs){
fp <- file.path(d, f)
if(file.exists(fp)){
if(debug) cat(sprintf('Leyendo: %s\n', fp))
df <- safe_read_csv(fp)
# agregar columna mes con el nombre de la carpeta (sin prefijo CSV_)
mes_name <- sub('CSV_', '', basename(d))
df$mes <- mes_name
tablas_mes[[length(tablas_mes) + 1]] <- df
origen <- c(origen, mes_name)
}
}
if(length(tablas_mes) > 0){
# Usar rbind.fill-like behavior: convertir a data.frame con columnas unidas por nombre
# Implementación simple: usar plyr::rbind.fill si disponible, sino base with missing cols
if(requireNamespace('plyr', quietly = TRUE)){
combined <- plyr::rbind.fill(tablas_mes)
} else {
# Alinear columnas
all_cols <- unique(unlist(lapply(tablas_mes, names)))
tablas_adj <- lapply(tablas_mes, function(x){
missing <- setdiff(all_cols, names(x))
for(mc in missing) x[[mc]] <- NA
# ordenar columnas
x[all_cols]
})
combined <- do.call(rbind, tablas_adj)
rownames(combined) <- NULL
}
datos[[tools::file_path_sans_ext(f)]] <- combined
if(debug) cat(sprintf('Combinado %s: %d filas, %d columnas\n', tools::file_path_sans_ext(f), nrow(combined), ncol(combined)))
}
}
# ===================================================================
# Archivo 1: cargar los datos de la GEIH en una estructura de datos
#            y aplicar transformaciones para su posterior análisis
#
# 1) Buenas práctias y librerías
# 2) Cargar los datos de la GEIH
# 3) Renombrar variables
# 4) Determinar varibles por atrición (>40% atrición)
# 5) Creación de variables propias
# ===================================================================
# ===================================================================
# 1) Buenas prácticas y librerías
# ===================================================================
# Limpiar el entorno de R
rm(list = ls())
# Cargar librerías
require("pacman")
pload(readr, dplyr, plyr, stringr, tools) #TODO
# ---- Parámetros ----
debug <- TRUE
# ===================================================================
# 2) Cargar los datos de la GEIH
# ===================================================================
# Ruta relativa a las carpetas con datos (asumiendo este script en scripts/)
base_dir <- file.path('..','GEIH-2025')
# Construir dinámicamente la lista de carpetas CSV_*. Si no existe, avisar.
if(!dir.exists(base_dir)){
stop(sprintf('No existe el directorio esperado: %s. Ajusta `base_dir`.', base_dir))
}
mes_dirs_all <- list.dirs(path = base_dir, full.names = TRUE, recursive = FALSE)
mes_dirs <- mes_dirs_all[grepl('CSV_', basename(mes_dirs_all))]
if(length(mes_dirs) == 0){
stop(sprintf('No se encontraron carpetas con prefijo CSV_ dentro de %s', base_dir))
}
# Función utilitaria: leer un CSV con encoding y tipos flexibles
safe_read_csv <- function(path){
# usar readr::read_csv para lecturas más robustas
tryCatch(
readr::read_csv(path, locale = readr::locale(encoding = 'UTF-8'), show_col_types = FALSE),
error = function(e) readr::read_csv(path, locale = readr::locale(encoding = 'Latin1'), show_col_types = FALSE)
)
}
# Reunir lista de nombres de archivos (asumiendo mismos nombres en cada mes)
all_files <- unique(unlist(lapply(mes_dirs, function(d) basename(list.files(d, pattern = '\\.[Cc][Ss][Vv]$', full.names = FALSE)))))
# Lista para guardar tablas combinadas
datos <- list()
if(debug) cat(sprintf('Detectados %d archivos distintos en las carpetas de meses:\n', length(all_files)))
if(debug) print(all_files)
for(f in all_files){
# para cada archivo (por ejemplo 'Ocupados.CSV') leer la versión de cada mes y apilar
tablas_mes <- list()
origen <- c()
for(d in mes_dirs){
fp <- file.path(d, f)
if(file.exists(fp)){
if(debug) cat(sprintf('Leyendo: %s\n', fp))
df <- safe_read_csv(fp)
# agregar columna mes con el nombre de la carpeta (sin prefijo CSV_)
mes_name <- sub('CSV_', '', basename(d))
df$mes <- mes_name
tablas_mes[[length(tablas_mes) + 1]] <- df
origen <- c(origen, mes_name)
}
}
# Asignar a .GlobalEnv para que queden disponibles tras source() en RStudio
assign('datos', datos, envir = .GlobalEnv)
assign('missing_report', missing_report, envir = .GlobalEnv)
if(debug) cat('\nObjetos `datos` y `missing_report` guardados en el Global Environment.\n')
if(length(tablas_mes) > 0){
# Usar rbind.fill-like behavior: convertir a data.frame con columnas unidas por nombre
# Implementación simple: usar plyr::rbind.fill si disponible, sino base with missing cols
if(requireNamespace('plyr', quietly = TRUE)){
combined <- plyr::rbind.fill(tablas_mes)
} else {
# Alinear columnas
all_cols <- unique(unlist(lapply(tablas_mes, names)))
tablas_adj <- lapply(tablas_mes, function(x){
missing <- setdiff(all_cols, names(x))
for(mc in missing) x[[mc]] <- NA
# ordenar columnas
x[all_cols]
})
combined <- do.call(rbind, tablas_adj)
rownames(combined) <- NULL
}
datos[[tools::file_path_sans_ext(f)]] <- combined
if(debug) cat(sprintf('Combinado %s: %d filas, %d columnas\n', tools::file_path_sans_ext(f), nrow(combined), ncol(combined)))
}
}
# ===================================================================
# (continuación) 2) Reporte de porcentaje de ausencias por variable (NA o cadena vacía)
# ===================================================================
missing_report <- list()
is_missing_vec <- function(x){
if(is.factor(x)) x <- as.character(x)
if(is.character(x)){
return(is.na(x) | trimws(x) == '')
}
is.na(x)
}
for(name in names(datos)){
df <- datos[[name]]
n <- nrow(df)
if(n == 0){
missing_report[[name]] <- data.frame(variable = character(0), pct_missing = numeric(0), stringsAsFactors = FALSE)
next
}
vars <- names(df)
pct <- sapply(vars, function(v){
miss <- sum(is_missing_vec(df[[v]]))
round(100 * miss / n, 2)
})
missing_report[[name]] <- data.frame(variable = vars, pct_missing = as.numeric(pct), stringsAsFactors = FALSE)
}
# Al final, tenemos 'datos' (lista de tablas combinadas por tipo de archivo)
# y 'missing_report' con porcentajes de ausencia por variable para cada tabla.
if(debug){
cat('\nResumen de ausencias por tabla:\n')
for(nm in names(missing_report)){
cat(sprintf('\n== %s ==\n', nm))
dfm <- missing_report[[nm]]
# mostrar las variables con mayor ausencia (top 10)
dfm <- dfm[order(-dfm$pct_missing),]
print(utils::head(dfm, 20))
}
}
ls()
setwd()
ls()
install.packages("pacman")
require(pacman)
p_load(textir)
data("we8there", package = "textir")
dta<- we9thereCounts
require(pacman)
p_load(textir)
data("we8there", package = "textir")
dta<- we9thereCounts
dta<- we8thereCounts
class(dta)
dim(dta)
dta(1,)
pca <- prcomp(dta,scale=TRUE)
library(dplyr)
get_top_words <- function(loadings, comp, n = 10) {
tibble(
palabra = names(sort(abs(loadings[, comp]), decreasing = TRUE))[1:n],
peso = sort(loadings[, comp], decreasing = TRUE)[1:n]
)
}
top_pc1 <- get_top_words(pca$rotation, 1)
top_pc2 <- get_top_words(pca$rotation, 2)
top_pc1
View(top_pc1)
p_load(wordcloud)
# Nube para el primer componente (palabras con pesos positivos)
loadings_pc1 <- sort(pca$rotation[,1], decreasing = TRUE)
wordcloud(names(loadings_pc1), loadings_pc1, max.words = 40, colors = "steelblue")
wordcloud(names(loadings_pc1), loadings_pc1, max.words = 40, colors = "steelblue")
biplot(pca, scale = 0, cex = 0.6)
biplot(pca, scale = 0, cex = 0.6)
plot(pca$x[, 1], pca$x[, 2],
xlab = "PC1: tono positivo vs negativo",
ylab = "PC2: servicio vs comida",
main = "Documentos (reseñas) en el espacio PCA",
pch = 19, col = rgb(0, 0, 0.7, 0.4))
library(devtools)
install.packages("devtools")
install.packages("devtools")
install.packages("devtools")
install.packages("devtools")
library(devtools)
install_github("topepo/caret/pkg/caret")
# Buenas prácticas
rm(list = ls())
# NOTA: CAMBIAR DIRECTORIO
# setwd("C:/Users/Asuar/OneDrive/Escritorio/Libros Clases/Economía/Ciencia Datos y Econometria/Taller3-blog_casciari")
# Librerías
require(pacman)
p_load(tidyverse, stringr, dplyr, stringi, tm, stopwords) #TODO para procesar data
p_load(rvest, dplyr, udpipe) # para web scrapping
# Datos y primera visualización
db <- read.csv("stores/blog_casciari.csv")
colnames(Db)
colnames(db)
setwd("C:/Users/Asuar/OneDrive/Escritorio/Libros Clases/Economía/Ciencia Datos y Econometria/Taller3-blog_casciari")
db <- read.csv("stores/blog_casciari.csv")
# Leer la página web
url <- "https://www.lifeder.com/frases-palabras-argentinas/"
# Si esta URL falla, podrías usar otra fuente de expresiones argentinas:
# url <- "https://languagetool.org/insights/es/publicacion/palabras-argentinas/"
page <- read_html(url)
# E
palabras_raw <- page %>%
html_nodes("strong, b") %>%      # selecciona etiquetas strong o b
html_text() %>%
str_trim()
palabras_raw
palabras_filt <- palabras_raw %>%
tolower() %>%
unique() %>%
# quitar puntuación
str_replace_all("[[:punct:]]", "") %>%
# Filtrar y procesar; vacías, minuscula, duplicacdos, puntuación,
palabras_filt <- palabras_raw %>%
tolower() %>%
unique() %>%
# quitar puntuación
str_replace_all("[[:punct:]]", "") %>%
# Lematización usando udpipe
# Descargar y cargar modelo español
ud_model_file <- udpipe_download_model(language = "spanish")
# Filtrar y procesar; vacías, minuscula, duplicacdos, puntuación,
palabras_filt <- palabras_raw %>%
tolower() %>%
unique() %>%
# quitar puntuación
str_replace_all("[[:punct:]]", "")
ud_model_file <- udpipe_download_model(language = "spanish")
ud_model <- udpipe_load_model(ud_model_file$file_model)
df_arg <- data.frame(token = palabras_filt, stringsAsFactors = FALSE)
anot_arg <- udpipe_annotate(ud_model, x = df_arg$token)
anot_arg <- as.data.frame(anot_arg)
# Tomar el lemma de cada token
palabras_lemma <- anot_arg %>%
select(token, lemma) %>%
distinct() %>%
mutate(lemma = ifelse(is.na(lemma) | lemma == "", token, lemma)) %>%
pull(lemma) %>%
unique()
# Resultado final en una lista
stopwords_arg <- palabras_lemma
stopwords_arg
# Eliminamos tildes y caracteres especiales del español
cuentos <- stri_trans_general(str = db$cuento, id = "Latin-ASCII")
# Quitar "/n" y reemplazar por espacio
cuentos <- str_replace_all(cuentos, "\n"," ")
# Quitar últimas 5 palabras, pues son nombre autor y fecha
cuentos <- sapply(str_split(cuentos, "\\s+"), function(x) paste(head(x, -5), collapse = " "))
# Volver strings en objetos tipo Corpus para funciones con la librería
cuentos <- Corpus(VectorSource(cuentos))
# Quitar puntuacion, números, espacios dobles, y pasar todo a minuscula
cuentos <- tm_map(cuentos,content_transformer(removePunctuation))
cuentos <- tm_map(cuentos,content_transformer(removeNumbers))
cuentos <- tm_map(cuentos,content_transformer(stripWhitespace))
cuentos <- tm_map(cuentos, content_transformer(tolower))
# Lista stopwords español de dos fuentes diferentes y combinamos
lista_palabras1 <- stopwords(language = "es", source = "snowball")
lista_palabras2 <- stopwords(language = "es", source = "nltk")
lista_palabras <- union(lista_palabras1, lista_palabras2)
# TODO: podría quitarse nombres o cosas recurrentes
# (Nombres buscar palabras empiezan mayúsculas y revisar) (revisar todos terminan con Hérnan Casciari)
# También lenguaje argento
# Agregamos palabras específica
lista_palabras <- c(lista_palabras,"mientras")
cuentos<-tm_map(cuentos, removeWords, lista_palabras)
texto_df <- data.frame(texto = sapply(cuentos, as.character), stringsAsFactors = FALSE)
# Procesar con udpipe
anotaciones <- udpipe_annotate(modelo_esp, x = texto_df$texto)
# Procesar con udpipe
anotaciones <- udpipe_annotate(ud_model, x = texto_df$texto)
anotaciones <- as.data.frame(anotaciones)
anotaciones <- as.data.frame(anotaciones)
# Usar solo las lemas (en minúsculas)
cuentos_lematizados <- anotaciones %>%
group_by(doc_id) %>%
summarise(texto = paste(lemma, collapse = " ")) %>%
pull(texto)
# Volver a objeto Corpus
cuentos <- Corpus(VectorSource(cuentos_lematizados))
# Tokenizar (crear matriz documentos)
dtm_cuentos<-DocumentTermMatrix(cuentos)
View(dtm_cuentos)
dtm_cuentos[["j"]]
dtm_cuentos[["dimnames"]]
dtm_cuentos[["dimnames"]][["Terms"]]
dtm_cuentos[["dimnames"]][["Docs"]]
dtm_cuentos[["v"]]
cuentos_lematizados
p_load("tokenizers")
# Convertimos el corpus lematizado a vector de texto
textos_lem <- sapply(cuentos, as.character)
# Generamos n-gramas de 1, 2 y 3 palabras
tokens_unigramas <- tokenize_words(textos_lem)
tokens_bigramas  <- tokenize_ngrams(textos_lem, n = 2)
tokens_trigramas <- tokenize_ngrams(textos_lem, n = 3)
# Combinamos todos los tokens en un solo texto por documento
tokens_combinados <- mapply(function(u, b, t) {
paste(c(u, b, t), collapse = " ")
}, tokens_unigramas, tokens_bigramas, tokens_trigramas)
# Volvemos al formato Corpus para continuar el pipeline
cuentos <- Corpus(VectorSource(tokens_combinados))
# Tokenizar (crear matriz documentos)
dtm_cuentos<-DocumentTermMatrix(cuentos)
# Quitar sparsity; quitar palabras que no están en 95% docs
dtm_cuentos <- removeSparseTerms(dtm_cuentos, sparse = 0.90)
View(dtm_cuentos)
View(cuentos)
cuentos[["1"]][["content"]]
View(db)
db$cuento[1]
